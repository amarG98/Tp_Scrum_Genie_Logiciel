Nom du fichier d’origine : Doyle 2005 Automatic Categorization of Author Gender



Titre du papier  : Automatic Categorization of Author Gender

Auteur :  Automatic Categorization of Author Gender


Abstract de l’auteur : We present a method for automaticcategorization of author gender vian-gram analysis. Using a corpusof British student essays, experiments using character-level, wordlevel, and part-of-speech n-grams areperformed. The peak accuracy for allmethods is roughly equal, reaching amaximum of 8
Introduction : 
There are different subtasks of text classification and they can be divided into topic-based
and non-topic-based classification. The traditional text classification is topic-based and
a typical example is news classification. Recently, there has been an increasing activity in
the area of non-topic classification as well, e.g.,
in sub-tasks such as
1. genre classification (Finn and Kushmerick, 2003), (E. Stamatatos and Kokkinakis, 2000),
2. sentiment classification,
3. spam identification,
4. language and encoding identification, and
5. authorship attribution and plagiarism detection (Khmelev and Teahan, 2003).
Many algorithms have been invented for assessing the authorship of a given text. These
algorithms rely on the fact that authors use linguistic devices at every level—semantic, syntactic, lexicographic, orthographic and morphological (Ephratt, 1997)—to produce their

text. Typically, such devices are applied unconsciously by the author, and thus provide
a useful basis for unambiguously determining
authorship. The most common approach to
determining authorship is to use stylistic analysis that proceeds in two steps: first, specific style markers are extracted, and second,
a classification procedure is applied to the resulting description. These methods are usually
based on calculating lexical measures that represent the richness of the author’s vocabulary
and the frequency of common word use (Stamatatos et al., 2001). Style marker extraction
is usually accomplished by some form of nontrivial NLP analysis, such as tagging, parsing
and morphological analysis. A classifier is then
constructed, usually by first performing a nontrivial feature selection step that employs mutual information or Chi-square testing to determine relevant features.
However, there are several disadvantages of
this standard approach. First, techniques used
for style marker extraction are almost always
language dependent, and in fact differ dramatically from language to language. For example,
an English parser usually cannot be applied to
German or Chinese. Second, feature selection
is not a trivial process, and usually involves
setting thresholds to eliminate uninformative
features (Scott and Matwin, 1999). These decisions can be extremely subtle, because although rare features contribute less signal than
common features, they can still have an important cumulative effect (Aizawa, 2001). Third,
current authorship attribution systems invariably perform their analysis at the word level.
However, although word level analysis seems
to be intuitive, it ignores the fact that morphological features can also play an important
role, and moreover that many Asian languages
such as Chinese and Japanese do not have word
boundaries explicitly identified in text. In fact,

word segmentation itself is a difficult problem in Asian languages, which creates an extra
level of difficulty in coping with the errors this
process introduces. Additionally, the number
of authors is small in all reported experiments,
so the size of author-specific information is not
an issue. If the number of authors, or classes in
general, is large, we have to set a limit on the
author-specific information, i.e., on the author
profile.
In this paper, we propose a simple method
that avoids each of these problems.
Two important operations are:
1. choosing the optimal set of n-grams to be
included in the profile, and
2. calculating the similarity between two
profiles.
The approach does not depend on a specific
language, and it does not require segmentation
for languages such as Chinese or Thai. There
is no any text preprocessing or higher level processing required for character or word n-grams,
while the most complicated NLP tool used being a part-of-speech tagger used in two of the
experiments.
The small profile size is not important only
for efficiency reasons, but it is also a natural
mechanism for over-fitting control.


Corps : There are different subtasks of text classification and they can be divided into topic-basedand non-topic-based classification. The traditional text classification is topic-based anda typical example is news classification. Recently, there has been an increasing activity inthe area of non-topic classification as well, e.g.,in sub-tasks such as1. genre classification (Finn and Kushmerick, 2003), (E. Stamatatos and Kokkinakis, 2000),2. sentiment classification,3. spam identification,4. language and encoding identification, and5. authorship attribution and plagiarism detection (Khmelev and Teahan, 2003).Many algorithms have been invented for assessing the authorship of a given text. Thesealgorithms rely on the fact that authors use linguistic devices at every level—semantic, syntactic, lexicographic, orthographic and morphological (Ephratt, 1997)—to produce theirtext. Typically, such devices are applied unconsciously by the author, and thus providea useful basis for unambiguously determiningauthorship. The most common approach todetermining authorship is to use stylistic analysis that proceeds in two steps: first, specific style markers are extracted, and second,a classification procedure is applied to the resulting description. These methods are usuallybased on calculating lexical measures that represent the richness of the author’s vocabularyand the frequency of common word use (Stamatatos et al., 2001). Style marker extractionis usually accomplished by some form of nontrivial NLP analysis, such as tagging, parsingand morphological analysis. A classifier is thenconstructed, usually by first performing a nontrivial feature selection step that employs mutual information or Chi-square testing to determine relevant features.However, there are several disadvantages ofthis standard approach. First, techniques usedfor style marker extraction are almost alwayslanguage dependent, and in fact differ dramatically from language to language. For example,an English parser usually cannot be applied toGerman or Chinese. Second, feature selectionis not a trivial process, and usually involvessetting thresholds to eliminate uninformativefeatures (Scott and Matwin, 1999). These decisions can be extremely subtle, because although rare features contribute less signal thancommon features, they can still have an important cumulative effect (Aizawa, 2001). Third,current authorship attribution systems invariably perform their analysis at the word level.However, although word level analysis seemsto be intuitive, it ignores the fact that morphological features can also play an importantrole, and moreover that many Asian languagessuch as Chinese and Japanese do not have wordboundaries explicitly identified in text. In fact,word segmentation itself is a difficult problem in Asian languages, which creates an extralevel of difficulty in coping with the errors thisprocess introduces. Additionally, the numberof authors is small in all reported experiments,so the size of author-specific information is notan issue. If the number of authors, or classes ingeneral, is large, we have to set a limit on theauthor-specific information, i.e., on the authorprofile.In this paper, we propose a simple methodthat avoids each of these problems.Two important operations are:1. choosing the optimal set of n-grams to beincluded in the profile, and2. calculating the similarity between twoprofiles.The approach does not depend on a specificlanguage, and it does not require segmentationfor languages such as Chinese or Thai. Thereis no any text preprocessing or higher level processing required for character or word n-grams,while the most complicated NLP tool used being a part-of-speech tagger used in two of theexperiments.The small profile size is not important onlyfor efficiency reasons, but it is also a naturalmechanism for over-fitting control.2N-Gram AnalysisThe term ‘N-gram’ refers to a series of sequential tokens in a document. The series can beof length 1 (‘unigrams’), length 2 (‘bigrams’),etc, towards the generalized term “N-gram”.The tokens used can be words, letters, or anyother unit of information present throughoutthe document. This versatility allows N-gramanalysis techniques to be applied to other media: both images (Rickman and Rosin, 1996)and music (Doraisamy and Ruger, 2003) havebeen the focus of N-gram research.N-grams have been used in a wide variety ofsituations, including optical character recognition (Harding et al., 1997) and author attribution (Keselj et al., 2003). The techniqueinvolves the construction of a ‘profile’ — essentially a listing of the relative proportions ofeach potential N-gram. When an item is to beclassified, its profile is compared with knownones to determine the best match. The basic method of comparison is an N-dimensionaldistance measurement.The use of n-gram probability distributionand n-gram models in NLP is a relatively simple idea, but it has been found to be effectivein many applications. For example, characterlevel n-gram language models can be easily applied to any language, and even non-languagesequences such as DNA and music. Characterlevel n-gram models are widely used in textcompression—e.g., the PPM model (T. Belland Witten, 1990)—and have recently beenfound to be effective in text mining problemsas well (I. Witten and Teahan, 1999). Text categorization with n-gram models has also beenattempted by (Cavnar and Trenkle, 1994).3CorpusWe used a collection of student essays from theBritish Academic Written English (BAWE)corpus (Nesi et al., 2004). Only the pilot datafor this corpus was available; it nominally consisted of 500 essays, though not all of thesewere suitable for inclusion. The metadata included for each essay consisted of informationsuch as author gender, first language, the gradereceived etc.Two essays were simply not present; others did not have metadata present indicatingauthor gender. After these unacceptable essays were excluded, 495 were left in the set.Within these, the average document lengthwas 2,812 words or 17,994 characters, with1,391,710 words and 8,907,064 characters total.4Methodology4.1 Profile GenerationFor each experiment, an individual profile wascreated for each document in the test set using the Perl module Text::NGrams. The cutoffpoint for each individual profile was 100,000N-grams; as no document had this number ofunique N-grams, this implies that the profilefor each document was complete. Profiles werecreated using character, word, and part-ofspeech tags as the tokens to be profiled. In thelatter case, an additional experiment was performed after replacing non-function tags withan asterisk. Profiles were generated for Ngrams of size 1 through 5 inclusive, with thatsize being the limit of computational feasibility. No pre-processing of the data was performed; the documents were left as found inthe corpus.4.2Part-of-Speech TaggingAdditional copies of the text were generatedwith words replaced by their part-of-speechtag. The tagging was performed automaticallyusing the Perl module Lingua::EN::Tagger, asecond-order Hidden Markov Model-based tagger. A further copy of the text was madewith all non-function words removed, underthe assumption that treating content-bearingwords would not little meaning with respectto style. They were arbitrarily replaced by anasterisk. The speech categories considers asfunction words were: prepositions, pronouns,conjunctions, question adverbs (e.g. ‘when’),interjections, and determiners.4.3Training and Testing Sets20% each of the male and female lists, roundedup, were randomly selected; these documentswould constitute the test set. There were 42male and 58 female-authored texts in this set,for a total of 100 essays. The remaining essayswere taken as the training set.The ‘male’ and ‘female’ essays within this setwere listed, and for each list, the profiles of thatlist’s members were combined. The combinedprofiles were then normalized so as to have asum N-gram count equal to 1. See Table 1for a sample of the data produced. This stepwas performed for all N-gram sizes for whichprofiles had been generated.Table 1: Top five character bigrams from thefemale training set, showing both normalizedand unnormalized values. Data has beentruncated for presentation.4.4Determining Closest ProfileFor each of the 100 documents in the test set, a‘distance’ measurement was calculated to thetrained ‘male’ and ‘female’ profiles. The distance between two profiles was calculated as in(Keselj et al., 2003); the exact formula is givenin equation 1.Xn ² profilesETSTHAUnnormalized0.032740.027430.024800.022770.019451212211015429182784306720012(p1 (n) − p2 (n))p1 (n) + p2 (n)¶2(1)Lower distances indicate a closer match; foreach essay, the lower distance was printed asthe system’s guess. The output was recordedand tested for accuracy, the results of whichcan be found in the next section.The experiment was repeated for variousprofile cutoff lengths; in each case, the mergedtest profile was simply truncated after a givennumber of entries and the distance measurements re-run. Note that this will have no effectonce the cutoff length exceeds the maximumprofile length, as there will be no items to betruncated at that point.5Results5.1Character N-GramsBoth male and female authors had spaces astheir most frequently-used character, followedby e,t,i, and a. Only minor differences followed thereafter — the profiles were very similar. This is to be expected, as are the poorresults for unigrams in this category.An increase in the n size provided a noticeable improvement, reaching a peak accuracyof 76% is reached for an N of 4 and an L of20,000.Table 2: Results using character-based extractionProfile LengthNormalizedµ100100050001000020000N-Gram Size341251%51%51%51%51%67%69%69%69%69%58%64%74%42%42%59%63%73%74%76%558%68%70%71%72%5.2Word N-GramsThe female authors appeared to have a slightlyhigher vocabulary than the male authors, withunique word counts of 31734 and 30186 respectively. The different rises for word pairs, with277769 unique word pairs within the femaletraining set, compared to 237417 in the male.This effect may be partially explained by thelarger number of female-authored documents.In general, the word-based categorizationwas highly successful, achieving a peak accuracy of 81% is reached for an N of 4 and an Lof 10,000–20,000.Table 3: Results using word-based extractionProfile Length10010002000500010000150005.3N-Gram Size341264%70%75%74%73%73%62%76%75%71%71%70%73%72%73%74%75%73%71%77%77%73%81%81%5.4 Function Word N-GramsIt has been also previously suggested that function words may be a strong determiner of author characteristics (Zhao and Zobel, 2005).To test this, the experiment was run again onprofiles for which non-function words had beenreplaced by an asterisk. The results of our testmay be seen in Table 5.4.As with the full part-of-speech profiles, apeak accuracy of 76% is reached. This time,it is for an N of 4 and an L of 1,000. Whilethe peak is the same, overall accuracy is lowerthan in Table 5.3.Table 5: Results using part-of-speech extraction, with non-function words replaced by anasterisk5Profile Length65%74%74%74%74%77%10050010002000500010000N-Gram Size341242%42%42%42%42%42%60%58%58%58%58%58%58%72%67%64%42%42%62%67%76%73%72%71%563%61%59%59%70%72%Part-of-Speech N-GramsIt has been suggested (Argamon et al., 2003)that part-of-speech N-grams can ‘efficiently encode syntactical information’, and that thismay be of use in style classification. This isnot unreasonable; the same source providesevidence for gender-based trends in part-ofspeech tags. Specifically, the results for Table5.3 shows the results for these. A peak accuracy of 76% is reached for an N of 5 and anL of 5,000. This is roughly comparable to theother results in this study.Table 4: Results using part-of-speech extractionProfile Length10050010002000500010000N-Gram Size341242%42%42%42%42%42%64%63%62%58%58%58%61%68%64%69%66%67%52%68%66%68%71%72%566%64%65%70%76%74%6ConclusionWe have presented a method for automaticidentification of author gender based on ngram profiles. The method is successful on thiscorpus; it would be desirable to try it on others to determine the versatility. Because no information specific to this experiment has beenused, it is likely that the techniques would beequally-applicable to other data sets. Further,the technique is not language-specific, suggesting is is probably applicable across languages.The use of part-of-speech tags had no substantial effect on the results, showing only aslight decrease. It is possible that with a moreaccurate tagger better results would be found.Although simple, in this case N-gram analysis performs on par with other techniques,achieving a peak accuracy of 81%. For comparative purposes, (Koppel et al., 2002) claiman accurate of ‘approximately 80%’.
Conclusion : We have presented a method for automaticidentification of author gender based on ngram profiles. The method is successful on thiscorpus; it would be desirable to try it on others to determine the versatility. Because no information specific to this experiment has beenused, it is likely that the techniques would beequally-applicable to other data sets. Further,the technique is not language-specific, suggesting is is probably applicable across languages.The use of part-of-speech tags had no substantial effect on the results, showing only aslight decrease. It is possible that with a moreaccurate tagger better results would be found.Although simple, in this case N-gram analysis performs on par with other techniques,achieving a peak accuracy of 81%. For comparative purposes, (Koppel et al., 2002) claiman accurate of ‘approximately 80%’.
Discussion : We would like to thank the maintainers of theBAWE corpus for providing access to the pilotdata used in this article. We would also like toacknowledge the contribution of three anonymous reviewers, whose feedback has been helpful.This research is supported by the NaturalSciences and Engineering Research Council ofCanada.
Biblio : A. Aizawa. 2001.
 Linguistic techniques to improve the performance of automatic text categorization. In Proceedings 6th NLP Pac. Rim Symp. NLPRS-01. Shlomo Argamon, Moshe Koppel, Jonathan Fine, and Anat Rachel Shimoni. 2003.
 Gender, genre, and writing style in formal written texts. Text, 23(3):321–346. W. Cavnar and J. Trenkle. 1994.
 N-gram-based text categorization. In Proceedings SDAIR-94. Shyamala Doraisamy and Stefan Ruger. 2003.
 Robust polyphonic music retrieval with ngrams. Journal of Intelligent Information Systems, 21(1):53–70, July. N. Fakotakis E. Stamatatos and G. Kokkinakis. 2000.
 Automatic text categorization in terms of genre and author. Computational Linguistics, 26(4):471–495. M. Ephratt. 1997.
 Authorship attribution - the case of lexical innovations. In Proc. ACH-ALLC97. Aidan Finn and Nicholas Kushmerick. 2003.
 Learning to classify documents according to genre. In IJCAI-03 Workshop on Computational Approaches to Style Analysis and Synthesis. Stephen M. Harding, W. Bruce Croft, and C. Weir. 1997.
 Probabilistic retrieval of ocr degraded text using n-grams. Probabilistic Retrieval of OCR Degraded Text Using N-Grams, 1324:345–359. M. Mahoui I. Witten, Z. Bray and W. Teahan. 1999.
 Text mining: A new frontier for lossless compression. In Proceedings of the IEEE Data Compression Conference (DCC). Vlado Keselj, Fuchun Peng, Nick Cercone, and Calvin Thomas. 2003.
 N-gram-based author profiles for authorship attribution. Proceedings of the Conference Pacific Association for Computational Linguistics PACLING’03, August.  D. Khmelev and W. Teahan. 2003.
 A repetition based measure for verification of text collections and for text categorization. In SIGIR’2003, Toronto, Canada. Moshe Koppel, Shlomo Argamon, and Anat Rachel Shimoni. 2002.
 Automatically categorizing written texts by author gender. Literary and Linguistic Computing, 17(4):401–412. Hilary Nesi, Gerard Sharpling, and Lisa Ganobcsik-Williams. 2004.
 Student papers across the curriculum: Designing and developing a corpus of british student writing. Computers and Composition, 21(4):439–450. R. Rickman and P. Rosin. 1996.
 Content-based image retrieval using colour n-grams. IEEE Colloquium on Intelligent Image Databases, pages 15/1–15/6. S. Scott and S. Matwin. 1999.
 Feature engineering for text classification. In Proceedings ICML-99. E. Stamatatos, N. Fakotakis, and G. Kokkinakis. 2001.
 Computer-based authorship attribution without lexical measures. Computers and the Humanities, 35:193–214. J. Cleary T. Bell and I. Witten. 1990.
 Text Compression. Prentice Hall. Ying Zhao and Justin Zobel. 2005.
 Effective and scalable authorship attribution using function words. The 2nd Asia Information Retrieval Symposium.  