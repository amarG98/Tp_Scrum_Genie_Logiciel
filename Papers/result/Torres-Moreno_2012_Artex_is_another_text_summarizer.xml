<article><preamble>Torres-Moreno 2012 Artex is another text summarizer</preamble><titre>Artex is AnotheR TEXt summarizer</titre><auteur>Juan-Manuel Torres-Moreno1,2
1

arXiv:1210.3312v1 [cs.IR] 11 Oct 2012

Laboratoire Informatique d&#8217;Avignon,
BP 91228 84911, Avignon, Cedex 09, France
juan-manuel.torres@univ-avignon.fr
2
&#201;cole Polytechnique de Montr&#233;al,
CP. 6128 succursale Centre-ville, Montr&#233;al, Qu&#233;bec, Canada
</auteur><abstract>This paper describes Artex, another algorithm for Automatic Text Summarization.In order to rank sentences, a simple inner product is calculated between each sentence, adocument vector (text topic) and a lexical vector (vocabulary used by a sentence). Summaries are then generated by assembling the highest ranked sentences. No ruled-basedlinguistic post-processing is necessary in order to obtain summaries. Tests over severaldatasets (coming from Document Understanding Conferences (DUC), Text Analysis Conference (TAC), evaluation campaigns, etc.) in French, English and Spanish have shownthat Artex summarizer achieves interesting results.</abstract><introduction>
Automatic Text Summarization (ATS) is the process to automatically generate a compressed
version of a source document [15]. Query-oriented summaries focus on a user&#8217;s request, and
extract the information related to the specified topic given explicitly in the form of a query
[2]. Generic mono-document summarization tries to cover as much as possible the information content. Multi-document summarization is a oriented task to create a summary from
a heterogeneous set of documents on a focused topic. Over the past years, extensive experiments on query-oriented multi-document summarization have been carried out. Extractive
Summarization produces summaries choosing a subset of representative sentences from original
documents. Sentences are ordered and then assembled according to their relevance to generate
the final summary [10].
This article introduces a new method of summarization based in sentences extraction on
Vector Space Model (VSM). We score each sentence by calculating their inner product with a
pseudo-sentence vector and a pseudo-word vector. Results show that Artex not only preserves
the content of the summaries generated using this new representation, but often, surprisingly
the performance can be improved. Artex could be an interesting and simple algorithm using
the extractive summarization paradigm. Our tests on trilingual corpora (English, Spanish
and French) evaluated by the Fresa algorithm (without human references) confirm the good
performance of Artex.
In this paper, related work is given in Section 2. Section 3 presents the new algorithm of
Automatic Text Summarization. Experiments are presented in Section 4, followed by Results
in Section 5 and Conclusions in Section 6.
1

</introduction><corps>Automatic Text Summarization (ATS) is the process to automatically generate a compressedversion of a source document [15]. Query-oriented summaries focus on a user&#8217;s request, andextract the information related to the specified topic given explicitly in the form of a query[2]. Generic mono-document summarization tries to cover as much as possible the information content. Multi-document summarization is a oriented task to create a summary froma heterogeneous set of documents on a focused topic. Over the past years, extensive experiments on query-oriented multi-document summarization have been carried out. ExtractiveSummarization produces summaries choosing a subset of representative sentences from originaldocuments. Sentences are ordered and then assembled according to their relevance to generatethe final summary [10].This article introduces a new method of summarization based in sentences extraction onVector Space Model (VSM). We score each sentence by calculating their inner product with apseudo-sentence vector and a pseudo-word vector. Results show that Artex not only preservesthe content of the summaries generated using this new representation, but often, surprisinglythe performance can be improved. Artex could be an interesting and simple algorithm usingthe extractive summarization paradigm. Our tests on trilingual corpora (English, Spanishand French) evaluated by the Fresa algorithm (without human references) confirm the goodperformance of Artex.In this paper, related work is given in Section 2. Section 3 presents the new algorithm ofAutomatic Text Summarization. Experiments are presented in Section 4, followed by Resultsin Section 5 and Conclusions in Section 6.12Related worksResearch in Automatic Text Summarization was introduced by H.P. Luhn in 1958 [9]. In thestrategy proposed by Luhn, the sentences are scored for their component word values as determined by tf*idf-like weights. Scored sentences are then ranked and selected from the topuntil some summary length threshold is reached. Finally, the summary is generated by assembling the selected sentences in the original source order. Although fairly simple, this extractivemethodology is still used in current approaches. Later on, [3] extended this work by adding simple heuristic features such as the position of sentences in the text or some key phrases indicatethe importance of the sentences. As the range of possible features for source characterizationwidened, choosing appropriate features, feature weights and feature combinations have becomea central issue.A natural way to tackle this problem is to consider sentence extraction as a classificationtask. To this end, several machine learning approaches that uses document-summary pairshave been proposed [6, 12], An hybrid method mixing statistical and linguistics algorithms ispresented in [1]. [10] and [15] propose a good state-of-art of Automatic Text Summarizationtasks and algorithms.2.1Document Pre-processingThe first step to represent documents in a suitable space is the pre-processing. As we useextractive summarization, documents have to be chunked into cohesive textual segments thatwill be assembled to produce the summary. Pre-processing is very important because theselection of segments is based on words or bigrams of words. The choice was made to splitdocuments into full sentences, in this way obtaining textual segments that are likely to begrammatically corrects. Afterwards, sentences pass through several basic normalization stepsin order to reduce computational complexity.The process is composed by the following steps:1. Sentence splitting. Simple rule-based method is used for sentence splitting. Documentsare chunked at the period, exclamation and question mark.2. Sentence filtering. Words lowercased and cleared up from sloppy punctuation. Wordswith less than 2 occurrences (f &lt; 2) are eliminated (Hapax legomenon presents once in adocument). Words that do not carry meaning such as functional or very common wordsare removed. Small stop-lists (depending of language) are used in this step.3. Word normalization. Remaining words are replaced by their canonical form usinglemmatization, stemming, ultra-stemming or none of them (raw text). Four methods ofnormalization were applied after filtering:&#8226; Lemmatization by simples dictionaries of morphological families. These dictionarieshave 1.32M, 208K and 316K words-entries in Spanish, English and French, respectively.&#8226; Porter&#8217;s Stemming, available at Snowball (web site http://snowball.tartarus.org/texts/stemmersoverview.html) for English, Spanish, French among other languages.&#8226; Ultra-stemming. This normalization seems be very efficient and it produces a compact matrix representation [16]. Ultra-stemming consider only the n first letters ofeach word. For example, in the case of ultra-stemming (first letter, Fix1 ), inflectedverbs like &#8220;sing&#8221;, &#8220;song&#8221;, &#8220;sings&#8221;, &#8220;singing&#8221;... or proper names &#8220;smith&#8221;, &#8220;snowboard&#8221;,&#8220;sex&#8221;,... are replaced by the letter &#8220;s&#8221;.4. Text Vectorization. Documents are vectorized in a matrix S[P &#215;N ] of P sentences andN columns. Each element si,j represents the occurrences of an object j (a letter in thecase of ultra-stemming, a word in the case of lemmatization or a stem for stemming),j = 1, 2, ..., N in the sentence i, i = 1, 2, ..., P .3AnotheR TEXt summarizer (Artex)Artex1 is a simple extractive algorithm for Automatic Text Summarization. The main idea isthe next one: First, we represent the text in a suitable space model (VSM). Then, we constructan average document vector that represents the average (the &#8220;global topic&#8221;) of all sentencesvectors. At the same time, we obtain the &#8220;lexical weight&#8221; for each sentence, i.e. the number ofwords in the sentence. After that, it is calculated the angle between the average document andeach sentence; narrow angles indicate that the sentences near of the &#8220;global topic&#8221; should beimportant and therefore extracted. See on the figure 1 the VSM of words: P vector sentencesand the average &#8220;global topic&#8221; are represented in a N dimensional space of words.w1VSM wordss1b Global topics2&#945;si SentencewNs3wjsPAngle cos &#945;=(b &#8226; s )/||b|| ||s ||iiFigure 1: The &#8220;global topic&#8221; in a Vector Space Model of N words.Next, a score for each sentence is calculated using their proximity with the &#8220;global topic&#8221;and their &#8220;lexical weight&#8221;. In the figure 2, the &#8220;lexical weight&#8221; is represented in a VSM of Psentences.Finally, the summary is generated concatenating the sentences with the highest scores following their order in the original document.1 InFrench, Artex est un Autre R&#233;sumeur TEXtuel.VSM sentencesw1s1w2a Lexical weightwjsPwNsiFigure 2: The &#8220;lexical weight&#8221; in a Vector Space Model of P sentences.3.1AlgorithmFormally, Artex algorithm computes the score of each sentence by calculating the inner productbetween a sentence vector, an average pseudo-sentence vector (the &#8220;global topic&#8221;) and an averagepseudo-word vector (the &#8220;lexical weight&#8221;).Once a pre-processing (word normalization and filtering of stop words) is completed, it iscreated a matrix S[P &#215;N ] , using the Vector Space Model, that contains N words (or letters) andP sentences.Let si = (s1 , s2 , ..., sN ) be a vector of the sentence i, i = 1, 2, ..., P . We defined ~a the averagepseudo-word vector, as the average number of occurrences of N words used in the sentence i:(1)ai =1 Xsi,jN jand ~b the average pseudo-sentence vector as the average number of occurrences of each word jused trough the P sentences:(2)bj =1 Xsi,jP iThe score or weight of each sentence si is calculated as follows:(3)&#63723;&#63734;X1&#63725;score(si ) = ~s &#215; ~b &#215; ~a =si,j &#215; bj &#63736; &#215; ai ; i = 1, 2, ..., P ; j = 1, 1, ..., NNPjThe score(&#8226;) computed by equation 3 must be normalized between the interval [0,1]. Thecalculation of ~s &#215; ~b indicates the proximity between the sentence ~s and the average pseudosentence ~b. The product (~s &#215; ~b) &#215; ~a weigh this proximity using the average pseudo-word ai .If a sentence si is near of ~b and their corresponding element ai has a high value, si will have,therefore, a high score. Moreover, a sentence i far of main topic (i.e. ~si &#215; ~b is near 0) or a lessinformative sentence i (i.e. ai are near 0) will have a low score.In computational terms, it is not really necessary to divide the scalar product by the constant1~ s/|~b||~s| between ~b and ~s is the same if we use ~b = ~b0 = P si,j .iN P , because the angle &#945; = arccos b.~The element ai is only a scale factor that does not modify &#945;.0In fact, if the matrix S[P &#215;N ] is approximated to a binary matrix2 S[P&#215;N ] , where each element1s0 = {0, 1} has a probability of p = , we can normalize vectors ~a, ~b and matrix S, as follows:i,j2|~a| =(4)P qXs0i,j 2 =|~b| =(5)s0i,j 2 =|~si | =N qXjN qX({0, 1}N )2 =&#8730;NPjj(6)&#8730;({0, 1}P )2 = N PiiN qXP qXs0i,j 2 =N pX{0, 1}2 = NjVectors then will be represented in hyper-spheres of N or P dimensions, and the normalizedscore&#8217; in this space would be:score&#8217;(si )(7)==&#63723;&#63734;!X~b~a~s1&#8730; &#63725;&#215;si,j &#215; bj &#63736; &#215; ai&#215;= &#8730;|~s| |~b||~a|N NPN Pj&#63723;&#63734;X1&#63725;&#8730;si,j &#215; bj &#63736; &#215; ai ; i = 1, 2, ..., P ; j = 1, 2, ..., NN 5P 3j&#8730;However, the term 1/ N 5 P 3 is a constant value (i.e. a simple scale factor), and then thescore(&#8226;) calculated using the equation 3) and the score&#8217;(&#8226;) using the equation 7, are bothequivalent.4ExperimentsArtex algorithm described in the previous section has been implemented and evaluated incorpora in several languages.We have conducted our experimentation with the following languages, summarization tasks,summarizers and data sets: 1) Generic multi-document-summarization in English with thecorpus DUC&#8217;04; 2) Generic single-document summarization in Spanish with the corpus MedicinaCl&#237;nica and 3) Generic single document summarization in French with the corpus Pistes.We have applied the summarization algorithms and finally, the results have been evaluatedusing Fresa while processing times for each summarizer have been measured and compared.The following subsections present formally the details of the summarizers, corpora andevaluations studied in different experiments.2 This is a reasonable approximation in this context, because S[P &#215;N ] is a sparsed matrix with many termoccurrences equal to one or zero.4.1Other SummarizersTo compare the performances, two other summarization systems were used in our experiments:Cortex and Enertex. To be in the same conditions, these two systems have used exactly thesame textual representation based on Vector Space Model, described in Section 2.1.&#8226; Cortex is a single-document summarization system using several metrics and an optimaldecision algorithm [4, 14, 15, 18].&#8226; Enertex is a summarization system based in Textual Energy concept [5]: text is represented as a spin system where spins &#8593; represents words that their occurrences are f &gt; 1(spins &#8595; if the word is not present).4.2Summarization Corpora DescriptionTo study the impact of our summarizer, we used corpora in three languages: English, Spanishand French. The corpora are heterogeneous, and different tasks are representatives of Automatic Text Summarization: generic multi-document summary and mono-document guided bya subject.&#8226; Corpus in English. Piloted by NIST in Document Understanding Conference3 (DUC) theTask 2 of DUC&#8217;044 , aims to produce a short summary of a cluster of related documents.We studied generic multi-document-summarization in English using data from DUC&#8217;04.This corpus with 300K words (17 780 types) is compound of 50 clusters, 10 documentseach.&#8226; Corpus in Spanish. Generic single-document summarization using a corpus from thescientific journal Medicina Cl&#237;nica5 , which is composed of 50 medical articles in Spanish,each one with its corresponding author abstract. This corpus contains 125K words (9 657types).&#8226; Corpus in French. We have studied generic single-document summarization using theCanadian French Sociological Articles corpus, generated from the journal Perspectivesinterdisciplinaires sur le travail et la sant&#233; (Pistes)6 . It contains 50 sociological articlesin French, each one with its corresponding author abstract. This corpus contains near400K words (18 887 types).4.3Summaries Content EvaluationDUC conferences have introduced the ROUGE content evaluation [7], wich measures the overlap of n-grams between a candidate summary and reference summaries written by humans.However, to write the human summaries necessaries for ROUGE is a very expensive task.Recently metrics without references have been defined and experimented at DUC and TextAnalysis Conferences (TAC)7 workshops.Fresa content evaluation [13, 17] is similar to ROUGE evaluation, but human referencesummaries are not necessary. Fresa calculates the divergence of probabilities between the3 http://duc.nist.gov4 http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html5 http://www.elsevier.es/revistas/ctl_servlet?_f=7032&amp;revistaid=26 http://www.pistes.uqam.ca/7 www.nist.gov/taccandidate summary and the document source. Among these metrics, Kullback-Leibler (KL)and Jensen-Shannon (JS) divergences have been widely used by [8, 17] to evaluate the informativeness of summaries.In this article, we use Fresa, based in KL divergence with Dirichlet smoothing, like inthe 2010 and 2011 INEX edition [11], to evaluate the informative content of summaries bycomparing their n-gram distributions with those from source documents.Fresa only considered absolute log-diff between the terms occurrences of the source andthe summary. Let T be the set of terms in the source. For every t &#8712; T , we denote by CtT itsoccurrences in the source and CtS its occurrences in the summary.The Fresa package computed the divergence between the document source and the summaries as follows: SX   C TCttD(T ||S) =log |T | + 1 &#8722; log |S| + 1 (8)t&#8712;TTo evaluate the information content (the &#8220;quality&#8221;) of the generated summaries, after removing stop-words, several automatic measures were computed: Fresa1 (Unigrams of singlestems), Fresa2 (Bigrams of pairs of consecutive stems), FresaSU 4 (Bigrams with 2-gaps alsomade of pairs of consecutive stems) and finally, hFresai, i.e. the average of all Fresa values.The Fresa values (scores) are normalized between 0 and 1. High Fresa values mean lessdivergence regarding the source document summary, reflecting a greater amount of informationcontent. All summaries produced by the systems were evaluated automatically using Fresapackage.5ResultsIn this section we present the results for each corpus with different summarizers and the several normalization strategies used. Based on these results, firstly, we have verified that ultrastemming improves the performance of summarizers. Secondly, we show that Artex is a systemthat has a similar performances &#8211;in terms of information content and processing times&#8211; to otherstate-of-art summarizers.5.1Content evaluation&#8226; English corpus. Figure 3 shows the performance of the three summarizers using Fix1 ,stemming and lemmatization. Results show that ultra-stemming improves the score of thethree automatic summarizer systems. Artex and Cortex expose a similar performancesin information content.!!"# $Figure 3: Histogram plot of content evaluation for corpus DUC&#8217;04 Task 2, with hFresai measures, for each summarizer and each normalization.&#8226; Spanish corpus. Spanish is a language with a greater variability than English. Results infigure 4 shown that Artex summarizer outperforms Cortex and Enertex if stemmingor lemmatization are used as normalization.Figure 4: Histogram plot of content evaluation for Spanish corpus Medicina Cl&#237;nica withhFresai scores for each summarizer.&#8226; French corpus. French is a language with a large variability too. Figure 5 shows thescore hFresai on the French corpus Pistes. Results show a similar behavior: Ultrastemming improves the score of the three automatic summarization systems used. Inparticular, the efficacy of Artex is less sensible to word normalization than others summarizers.! "!#$%&amp;'()#! "Figure 5: Histogram plot of content evaluation for French corpus Pistes with hFresai scoresfor each summarizer.5.2Processing Times EvaluationTable 1 shows processing times for each corpus, following the normalization method for Cortex,Artex and Enertex summarizers8 . Processing times of ultra-stemming Fix1 are shortercompared to all others methods. By example, Cortex is a very fast summarizer with O(log &#961;2 )(where &#961; = P &#215; N ), and processing times for stemming and Fix1 are close. In other hand,Enertex summarizer has a complexity of O(&#961;2 ), then it needs more time to process the samecorpus. Performances of Artex algorithm remain close to Cortex.NormalizationLemmatizationStemmingfix1Summarizer Average Time(all corpora)Cortex Artex Enertex1.60&#8217;2.50&#8217;10.42&#8217;0.54&#8217;1.29&#8217;9.47&#8217;0.32&#8217;0.40&#8217;4.25&#8217;Table 1: Statistics of processing times (in minutes) of three summarizers over three corpora.8 All times are measured in a 7.8 GB of RAM computer, Core i7-2640M CPU @ 2.80GHz &#215; 4 processor,running under 32 bits GNU/Linux (Ubuntu Version 12.04).6ConclusionsIn this article we have introduced and tested a simple method for Automatic Text Summarization. Artex is a fast and very simple algorithm based in VSM model and the extractiveparadigm. The method uses a matrix representation to calculate a normalized score for eachsentence, using the inner product of pseudo-(sentences|words) vectors. The algorithm retainsthe salient information of each sentence of document. An important aspect of our approach isthat it does not requires linguistic knowledge or resources which makes it a simple and efficientsummarizer method to tackle the issue of Automatic Text Summarization.Summaries generated by Artex system are pertinents. The results obtained on corporain English, Spanish and French show that Artex can achieve good results for content quality.Tests with other corpora (DUC and TAC evaluation campaigns, INEX, etc.) in mono-andmulti-document guided by a subject, using content evaluation with (ROUGE evaluations) orwithout reference summaries still in progress.</corps><conclusion>s in Section 6.12Related worksResearch in Automatic Text Summarization was introduced by H.P. Luhn in 1958 [9]. In thestrategy proposed by Luhn, the sentences are scored for their component word values as determined by tf*idf-like weights. Scored sentences are then ranked and selected from the topuntil some summary length threshold is reached. Finally, the summary is generated by assembling the selected sentences in the original source order. Although fairly simple, this extractivemethodology is still used in current approaches. Later on, [3] extended this work by adding simple heuristic features such as the position of sentences in the text or some key phrases indicatethe importance of the sentences. As the range of possible features for source characterizationwidened, choosing appropriate features, feature weights and feature combinations have becomea central issue.A natural way to tackle this problem is to consider sentence extraction as a classificationtask. To this end, several machine learning approaches that uses document-summary pairshave been proposed [6, 12], An hybrid method mixing statistical and linguistics algorithms ispresented in [1]. [10] and [15] propose a good state-of-art of Automatic Text Summarizationtasks and algorithms.2.1Document Pre-processingThe first step to represent documents in a suitable space is the pre-processing. As we useextractive summarization, documents have to be chunked into cohesive textual segments thatwill be assembled to produce the summary. Pre-processing is very important because theselection of segments is based on words or bigrams of words. The choice was made to splitdocuments into full sentences, in this way obtaining textual segments that are likely to begrammatically corrects. Afterwards, sentences pass through several basic normalization stepsin order to reduce computational complexity.The process is composed by the following steps:1. Sentence splitting. Simple rule-based method is used for sentence splitting. Documentsare chunked at the period, exclamation and question mark.2. Sentence filtering. Words lowercased and cleared up from sloppy punctuation. Wordswith less than 2 occurrences (f &lt; 2) are eliminated (Hapax legomenon presents once in adocument). Words that do not carry meaning such as functional or very common wordsare removed. Small stop-lists (depending of language) are used in this step.3. Word normalization. Remaining words are replaced by their canonical form usinglemmatization, stemming, ultra-stemming or none of them (raw text). Four methods ofnormalization were applied after filtering:&#8226; Lemmatization by simples dictionaries of morphological families. These dictionarieshave 1.32M, 208K and 316K words-entries in Spanish, English and French, respectively.&#8226; Porter&#8217;s Stemming, available at Snowball (web site http://snowball.tartarus.org/texts/stemmersoverview.html) for English, Spanish, French among other languages.&#8226; Ultra-stemming. This normalization seems be very efficient and it produces a compact matrix representation [16]. Ultra-stemming consider only the n first letters ofeach word. For example, in the case of ultra-stemming (first letter, Fix1 ), inflectedverbs like &#8220;sing&#8221;, &#8220;song&#8221;, &#8220;sings&#8221;, &#8220;singing&#8221;... or proper names &#8220;smith&#8221;, &#8220;snowboard&#8221;,&#8220;sex&#8221;,... are replaced by the letter &#8220;s&#8221;.4. Text Vectorization. Documents are vectorized in a matrix S[P &#215;N ] of P sentences andN columns. Each element si,j represents the occurrences of an object j (a letter in thecase of ultra-stemming, a word in the case of lemmatization or a stem for stemming),j = 1, 2, ..., N in the sentence i, i = 1, 2, ..., P .3AnotheR TEXt summarizer (Artex)Artex1 is a simple extractive algorithm for Automatic Text Summarization. The main idea isthe next one: First, we represent the text in a suitable space model (VSM). Then, we constructan average document vector that represents the average (the &#8220;global topic&#8221;) of all sentencesvectors. At the same time, we obtain the &#8220;lexical weight&#8221; for each sentence, i.e. the number ofwords in the sentence. After that, it is calculated the angle between the average document andeach sentence; narrow angles indicate that the sentences near of the &#8220;global topic&#8221; should beimportant and therefore extracted. See on the figure 1 the VSM of words: P vector sentencesand the average &#8220;global topic&#8221; are represented in a N dimensional space of words.w1VSM wordss1b Global topics2&#945;si SentencewNs3wjsPAngle cos &#945;=(b &#8226; s )/||b|| ||s ||iiFigure 1: The &#8220;global topic&#8221; in a Vector Space Model of N words.Next, a score for each sentence is calculated using their proximity with the &#8220;global topic&#8221;and their &#8220;lexical weight&#8221;. In the figure 2, the &#8220;lexical weight&#8221; is represented in a VSM of Psentences.Finally, the summary is generated concatenating the sentences with the highest scores following their order in the original document.1 InFrench, Artex est un Autre R&#233;sumeur TEXtuel.VSM sentencesw1s1w2a Lexical weightwjsPwNsiFigure 2: The &#8220;lexical weight&#8221; in a Vector Space Model of P sentences.3.1AlgorithmFormally, Artex algorithm computes the score of each sentence by calculating the inner productbetween a sentence vector, an average pseudo-sentence vector (the &#8220;global topic&#8221;) and an averagepseudo-word vector (the &#8220;lexical weight&#8221;).Once a pre-processing (word normalization and filtering of stop words) is completed, it iscreated a matrix S[P &#215;N ] , using the Vector Space Model, that contains N words (or letters) andP sentences.Let si = (s1 , s2 , ..., sN ) be a vector of the sentence i, i = 1, 2, ..., P . We defined ~a the averagepseudo-word vector, as the average number of occurrences of N words used in the sentence i:(1)ai =1 Xsi,jN jand ~b the average pseudo-sentence vector as the average number of occurrences of each word jused trough the P sentences:(2)bj =1 Xsi,jP iThe score or weight of each sentence si is calculated as follows:(3)&#63723;&#63734;X1&#63725;score(si ) = ~s &#215; ~b &#215; ~a =si,j &#215; bj &#63736; &#215; ai ; i = 1, 2, ..., P ; j = 1, 1, ..., NNPjThe score(&#8226;) computed by equation 3 must be normalized between the interval [0,1]. Thecalculation of ~s &#215; ~b indicates the proximity between the sentence ~s and the average pseudosentence ~b. The product (~s &#215; ~b) &#215; ~a weigh this proximity using the average pseudo-word ai .If a sentence si is near of ~b and their corresponding element ai has a high value, si will have,therefore, a high score. Moreover, a sentence i far of main topic (i.e. ~si &#215; ~b is near 0) or a lessinformative sentence i (i.e. ai are near 0) will have a low score.In computational terms, it is not really necessary to divide the scalar product by the constant1~ s/|~b||~s| between ~b and ~s is the same if we use ~b = ~b0 = P si,j .iN P , because the angle &#945; = arccos b.~The element ai is only a scale factor that does not modify &#945;.0In fact, if the matrix S[P &#215;N ] is approximated to a binary matrix2 S[P&#215;N ] , where each element1s0 = {0, 1} has a probability of p = , we can normalize vectors ~a, ~b and matrix S, as follows:i,j2|~a| =(4)P qXs0i,j 2 =|~b| =(5)s0i,j 2 =|~si | =N qXjN qX({0, 1}N )2 =&#8730;NPjj(6)&#8730;({0, 1}P )2 = N PiiN qXP qXs0i,j 2 =N pX{0, 1}2 = NjVectors then will be represented in hyper-spheres of N or P dimensions, and the normalizedscore&#8217; in this space would be:score&#8217;(si )(7)==&#63723;&#63734;!X~b~a~s1&#8730; &#63725;&#215;si,j &#215; bj &#63736; &#215; ai&#215;= &#8730;|~s| |~b||~a|N NPN Pj&#63723;&#63734;X1&#63725;&#8730;si,j &#215; bj &#63736; &#215; ai ; i = 1, 2, ..., P ; j = 1, 2, ..., NN 5P 3j&#8730;However, the term 1/ N 5 P 3 is a constant value (i.e. a simple scale factor), and then thescore(&#8226;) calculated using the equation 3) and the score&#8217;(&#8226;) using the equation 7, are bothequivalent.4ExperimentsArtex algorithm described in the previous section has been implemented and evaluated incorpora in several languages.We have conducted our experimentation with the following languages, summarization tasks,summarizers and data sets: 1) Generic multi-document-summarization in English with thecorpus DUC&#8217;04; 2) Generic single-document summarization in Spanish with the corpus MedicinaCl&#237;nica and 3) Generic single document summarization in French with the corpus Pistes.We have applied the summarization algorithms and finally, the results have been evaluatedusing Fresa while processing times for each summarizer have been measured and compared.The following subsections present formally the details of the summarizers, corpora andevaluations studied in different experiments.2 This is a reasonable approximation in this context, because S[P &#215;N ] is a sparsed matrix with many termoccurrences equal to one or zero.4.1Other SummarizersTo compare the performances, two other summarization systems were used in our experiments:Cortex and Enertex. To be in the same conditions, these two systems have used exactly thesame textual representation based on Vector Space Model, described in Section 2.1.&#8226; Cortex is a single-document summarization system using several metrics and an optimaldecision algorithm [4, 14, 15, 18].&#8226; Enertex is a summarization system based in Textual Energy concept [5]: text is represented as a spin system where spins &#8593; represents words that their occurrences are f &gt; 1(spins &#8595; if the word is not present).4.2Summarization Corpora DescriptionTo study the impact of our summarizer, we used corpora in three languages: English, Spanishand French. The corpora are heterogeneous, and different tasks are representatives of Automatic Text Summarization: generic multi-document summary and mono-document guided bya subject.&#8226; Corpus in English. Piloted by NIST in Document Understanding Conference3 (DUC) theTask 2 of DUC&#8217;044 , aims to produce a short summary of a cluster of related documents.We studied generic multi-document-summarization in English using data from DUC&#8217;04.This corpus with 300K words (17 780 types) is compound of 50 clusters, 10 documentseach.&#8226; Corpus in Spanish. Generic single-document summarization using a corpus from thescientific journal Medicina Cl&#237;nica5 , which is composed of 50 medical articles in Spanish,each one with its corresponding author abstract. This corpus contains 125K words (9 657types).&#8226; Corpus in French. We have studied generic single-document summarization using theCanadian French Sociological Articles corpus, generated from the journal Perspectivesinterdisciplinaires sur le travail et la sant&#233; (Pistes)6 . It contains 50 sociological articlesin French, each one with its corresponding author abstract. This corpus contains near400K words (18 887 types).4.3Summaries Content EvaluationDUC conferences have introduced the ROUGE content evaluation [7], wich measures the overlap of n-grams between a candidate summary and reference summaries written by humans.However, to write the human summaries necessaries for ROUGE is a very expensive task.Recently metrics without references have been defined and experimented at DUC and TextAnalysis Conferences (TAC)7 workshops.Fresa content evaluation [13, 17] is similar to ROUGE evaluation, but human referencesummaries are not necessary. Fresa calculates the divergence of probabilities between the3 http://duc.nist.gov4 http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html5 http://www.elsevier.es/revistas/ctl_servlet?_f=7032&amp;revistaid=26 http://www.pistes.uqam.ca/7 www.nist.gov/taccandidate summary and the document source. Among these metrics, Kullback-Leibler (KL)and Jensen-Shannon (JS) divergences have been widely used by [8, 17] to evaluate the informativeness of summaries.In this article, we use Fresa, based in KL divergence with Dirichlet smoothing, like inthe 2010 and 2011 INEX edition [11], to evaluate the informative content of summaries bycomparing their n-gram distributions with those from source documents.Fresa only considered absolute log-diff between the terms occurrences of the source andthe summary. Let T be the set of terms in the source. For every t &#8712; T , we denote by CtT itsoccurrences in the source and CtS its occurrences in the summary.The Fresa package computed the divergence between the document source and the summaries as follows: SX   C TCttD(T ||S) =log |T | + 1 &#8722; log |S| + 1 (8)t&#8712;TTo evaluate the information content (the &#8220;quality&#8221;) of the generated summaries, after removing stop-words, several automatic measures were computed: Fresa1 (Unigrams of singlestems), Fresa2 (Bigrams of pairs of consecutive stems), FresaSU 4 (Bigrams with 2-gaps alsomade of pairs of consecutive stems) and finally, hFresai, i.e. the average of all Fresa values.The Fresa values (scores) are normalized between 0 and 1. High Fresa values mean lessdivergence regarding the source document summary, reflecting a greater amount of informationcontent. All summaries produced by the systems were evaluated automatically using Fresapackage.5ResultsIn this section we present the results for each corpus with different summarizers and the several normalization strategies used. Based on these results, firstly, we have verified that ultrastemming improves the performance of summarizers. Secondly, we show that Artex is a systemthat has a similar performances &#8211;in terms of information content and processing times&#8211; to otherstate-of-art summarizers.5.1Content evaluation&#8226; English corpus. Figure 3 shows the performance of the three summarizers using Fix1 ,stemming and lemmatization. Results show that ultra-stemming improves the score of thethree automatic summarizer systems. Artex and Cortex expose a similar performancesin information content.!!"# $Figure 3: Histogram plot of content evaluation for corpus DUC&#8217;04 Task 2, with hFresai measures, for each summarizer and each normalization.&#8226; Spanish corpus. Spanish is a language with a greater variability than English. Results infigure 4 shown that Artex summarizer outperforms Cortex and Enertex if stemmingor lemmatization are used as normalization.Figure 4: Histogram plot of content evaluation for Spanish corpus Medicina Cl&#237;nica withhFresai scores for each summarizer.&#8226; French corpus. French is a language with a large variability too. Figure 5 shows thescore hFresai on the French corpus Pistes. Results show a similar behavior: Ultrastemming improves the score of the three automatic summarization systems used. Inparticular, the efficacy of Artex is less sensible to word normalization than others summarizers.! "!#$%&amp;'()#! "Figure 5: Histogram plot of content evaluation for French corpus Pistes with hFresai scoresfor each summarizer.5.2Processing Times EvaluationTable 1 shows processing times for each corpus, following the normalization method for Cortex,Artex and Enertex summarizers8 . Processing times of ultra-stemming Fix1 are shortercompared to all others methods. By example, Cortex is a very fast summarizer with O(log &#961;2 )(where &#961; = P &#215; N ), and processing times for stemming and Fix1 are close. In other hand,Enertex summarizer has a complexity of O(&#961;2 ), then it needs more time to process the samecorpus. Performances of Artex algorithm remain close to Cortex.NormalizationLemmatizationStemmingfix1Summarizer Average Time(all corpora)Cortex Artex Enertex1.60&#8217;2.50&#8217;10.42&#8217;0.54&#8217;1.29&#8217;9.47&#8217;0.32&#8217;0.40&#8217;4.25&#8217;Table 1: Statistics of processing times (in minutes) of three summarizers over three corpora.8 All times are measured in a 7.8 GB of RAM computer, Core i7-2640M CPU @ 2.80GHz &#215; 4 processor,running under 32 bits GNU/Linux (Ubuntu Version 12.04).6ConclusionsIn this article we have introduced and tested a simple method for Automatic Text Summarization. Artex is a fast and very simple algorithm based in VSM model and the extractiveparadigm. The method uses a matrix representation to calculate a normalized score for eachsentence, using the inner product of pseudo-(sentences|words) vectors. The algorithm retainsthe salient information of each sentence of document. An important aspect of our approach isthat it does not requires linguistic knowledge or resources which makes it a simple and efficientsummarizer method to tackle the issue of Automatic Text Summarization.Summaries generated by Artex system are pertinents. The results obtained on corporain English, Spanish and French show that Artex can achieve good results for content quality.Tests with other corpora (DUC and TAC evaluation campaigns, INEX, etc.) in mono-andmulti-document guided by a subject, using content evaluation with (ROUGE evaluations) orwithout reference summaries still in progress.</conclusion><discussion> </discussion><biblio> [1] Iria da Cunha, Silvia Fern&#225;ndez, Patricia Vel&#225;zquez-Morales, Jorge Vivaldi, Eric SanJuan, and Juan Manuel Torres-Moreno. A new hybrid summarizer based on vector space model, statistical physics and linguistics. In Proceedings of the 6th Mexican International Conference on Advances in Artificial Intelligence (MICAI&#8217;07), pages 872&#8211;882, Aguascalientes, Mexico, 2007. Springer-Verlag  
  [2] Harold Daum&#233; III. Practical structured learning techniques for natural language processing. PhD thesis, Los Angeles, CA, 2006  
  [3] H. P. Edmundson. New Methods in Automatic Extraction. Journal of the Association for Computing Machinery, 16(2):264&#8211;285, 1969  
  [4] B. Favre, F. B&#233;chet, P. Bellot, F. Boudin, M. El-B&#232;ze, L. Gillard, G. Lapalme, and J-M. TorresMoreno. The LIA-Thales summarization system at DUC-2006. In Proceedings of the Document Understanding Conference (DUC&#8217;06), Brooklyn, New York, United States, 2006. http://duc.nist.gov  
  [5] Silvia Fern&#225;ndez, Eric SanJuan, and Juan-Manuel Torres-Moreno. Textual Energy of Associative Memories: performants applications of Enertex algorithm in text summarization and topic segmentation. In Proceedings of the Mexican International Conference on Artificial Intelligence (MICAI&#8217;07), pages 861&#8211;871, Aguascalientes, Mexico, 2007. Springer-Verlag  
  [6] J. Kupiec, J. Pedersen, and F. Chen. A trainable document summarizer. In Proceedings of the 18th Conference ACM Special Interest Group on Information Retrieval (SIGIR&#8217;95), pages 68&#8211;73, Seattle, WA, United States, 1995. ACM Press, New York  
  [7] Chin-Yew Lin. ROUGE: A Package for Automatic Evaluation of Summaries. In Marie-Francine Moens and Stan Szpakowicz, editors, Proceedings of the Workshop Text Summarization Branches Out (ACL&#8217;04), pages 74&#8211;81, Barcelone, Spain, july 2004. ACL  
  [8] Annie Louis and Ani Nenkova. Automatic Summary Evaluation without Human Models. In First Text Analysis Conference (TAC&#8217;08), Gaithersburg, MD, United States, 17-19 November 2008  
  [9] H.P. Luhn. The Automatic Creation of Literature Abstracts. IBM Journal of Research and Development, 2(2):159&#8211;165, 1958  
  [10] I. Mani and M. Mayburi. Advances in Automatic Text Summarization. MIT Press, Cambridge, 1999  
  [11] Eric SanJuan, Patrice Bellot, V&#233;ronique Moriceau, and Xavier Tannier. Overview of the INEX 2010 Question Answering Track (QA@INEX). In Shlomo Geva, Jaap Kamps, Ralf Schenkel, and Andrew Trotman, editors, Comparative Evaluation of Focused Retrieval, volume 6932 of Lecture Notes in Computer Science, pages 269&#8211;281. Springer Berlin / Heidelberg, 2011  
   [12] Simone Teufel and Marc Moens. Sentence extraction as a classification task. In I. Mani and M. Maybury, editors, Proceedings of the ACL/EACL&#8217;97 Workshop on Intelligent Scalable Text Summarization, Madrid, Spain, 11 July 1997  
  [13] J.-M. Torres-Moreno, Horacio Saggion, I. da Cunha, P. Velazquez-Morales, and E. SanJuan. Evaluation automatique de r&#233;sum&#233;s avec et sans r&#233;ferences. In Proceedings de la conference Traitement Automatique des Langagues Naturelles (TALN&#8217;10), Montr&#233;al, QC, Canada, 19-23 July 2010  
  ATALA  
  [14] J.-M. Torres-Moreno, P.-L. St-Onge, M. Gagnon, M. El-B&#232;ze, and P. Bellot. Automatic Summarization System coupled with a Question-Answering System (QAAS). CoRR, abs/0905.2990, 2009  
  [15] Juan-Manuel Torres-Moreno. R&#233;sum&#233; automatique de documents: une approche statistique  
  Herm&#232;s-Lavoisier, Paris, 2011  
  [16] Juan-Manuel Torres-Moreno. Beyond Stemming and Lemmatization: Ultra-stemming to Improve Automatic Text Summarization. CoRR, arXiv:1209.3126 [cs.IR], 2012  
  [17] Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, and Eric SanJuan. Summary Evaluation With and Without References. Polibits: Research journal on Computer science and computer engineering with applications, 42:13&#8211;19, 2010  
  [18] Juan-Manuel Torres-Moreno, Patricia Vel&#225;zquez-Morales, and Jean-Guy Meunier. Cortex : un algorithme pour la condensation automatique des textes. In Proceedings of the Conference de l&#8217;Association pour la Recherche Cognitive, volume 2, pages 365&#8211;366, Lyon, France, 2001  
   </biblio></article>