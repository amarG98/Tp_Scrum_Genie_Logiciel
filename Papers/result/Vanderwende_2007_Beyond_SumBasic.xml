<article><preamble>Vanderwende 2007 Beyond SumBasic</preamble><titre>Information Processing and Management 43 (2007) 1606&#8211;1618</titre><auteur>www.elsevier.com/locate/infoproman

Beyond SumBasic: Task-focused summarization with
sentence simpli&#64257;cation and lexical expansion
Lucy Vanderwende
a

a,*

, Hisami Suzuki a, Chris Brockett a, Ani Nenkova

b,1

Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA
b
Columbia University, New York, NY 10027, USA

Received 18 July 2006; received in revised form 18 January 2007; accepted 22 January 2007
Available online 19 April 2007

</auteur><abstract>In recent years, there has been increased interest in topic-focused multi-document summarization. In this task, automatic summaries are produced in response to a speci&#64257;c information request, or topic, stated by the user. The systemwe have designed to accomplish this task comprises four main components: a generic extractive summarization system,a topic-focusing component, sentence simpli&#64257;cation, and lexical expansion of topic words. This paper details each of thesecomponents, together with experiments designed to quantify their individual contributions. We include an analysis of ourresults on two large datasets commonly used to evaluate task-focused summarization, the DUC2005 and DUC2006 datasets, using automatic metrics. Additionally, we include an analysis of our results on the DUC2006 task according to humanevaluation metrics. In the human evaluation of system summaries compared to human summaries, i.e., the Pyramidmethod, our system ranked &#64257;rst out of 22 systems in terms of overall mean Pyramid score; and in the human evaluationof summary responsiveness to the topic, our system ranked third out of 35 systems.&#211; 2007 Elsevier Ltd. All rights reserved.</abstract><introduction>In recent years, there has been increased interest in topic-focused multi-document summarization. In this
task, automatic summaries are produced in response to a speci&#64257;c information request, or topic, stated by
the user. In response to this interest and to stimulate research in this area, the National Institute of Standards
and Technology (NIST) conducted a series of workshops, the Document Understanding Conference2 (DUC),
to provide large-scale common data sets for the evaluation of systems. Participants in DUC2005 and
DUC2006 were provided with a topic and a set of relevant documents (newswire articles), and the task was
*

Corresponding author. Tel.: +1 425 706 5560; fax: +1 425 936 7329.
E-mail addresses: lucyv@microsoft.com (L. Vanderwende), hisamis@microsoft.com (H. Suzuki), chrisbkt@microsoft.com (C.
Brockett), anenkova@stanford.edu (A. Nenkova).
1
Present address: Stanford University, Stanford, CA 94305, USA.
</introduction><corps>In recent years, there has been increased interest in topic-focused multi-document summarization. In thistask, automatic summaries are produced in response to a speci&#64257;c information request, or topic, stated bythe user. In response to this interest and to stimulate research in this area, the National Institute of Standardsand Technology (NIST) conducted a series of workshops, the Document Understanding Conference2 (DUC),to provide large-scale common data sets for the evaluation of systems. Participants in DUC2005 andDUC2006 were provided with a topic and a set of relevant documents (newswire articles), and the task was*Corresponding author. Tel.: +1 425 706 5560; fax: +1 425 936 7329.E-mail addresses: lucyv@microsoft.com (L. Vanderwende), hisamis@microsoft.com (H. Suzuki), chrisbkt@microsoft.com (C.Brockett), anenkova@stanford.edu (A. Nenkova).1Present address: Stanford University, Stanford, CA 94305, USA.2http://duc.nist.gov.0306-4573/$ - see front matter &#211; 2007 Elsevier Ltd. All rights reserved.doi:10.1016/j.ipm.2007.01.023L. Vanderwende et al. / Information Processing and Management 43 (2007) 1606&#8211;16181607to produce an automatic summary of not more than 250 words in length. Consider the topic description forclusters D0652G and D0655A, for example:D0652G: Identify the world&#8217;s top banana producers and their levels of production. Describe the main markets for imports and relate any major issues in the industry.D0655A: Discuss the causes, e&#64256;ects, and treatment of childhood obesity. How widespread is it?In order to evaluate the summaries produced by the participants&#8217; systems, called peer summaries, DUC provides four human summaries, model summaries, for comparison.Several methods for summarization evaluation have been proposed. The automatic metric used in DUC isROUGE (Lin, 2004) speci&#64257;cally, ROUGE-2, which calculates the overlap in bigrams between the peer and thefour model summaries, and ROUGE-SU4, which calculates the bigram overlap, but allowing up to 4 words tobe skipped in order to identify a bigram match. Automatic metrics are useful as they potentially allow a comparison between di&#64256;erent system settings, as we show in Section 6.1. However, since we are primarily interested in maximizing the content of our system&#8217;s summaries, which, due to many issues of semanticrealization such as paraphrase, cannot always be captured by measuring bigram overlap with four model summaries, we also evaluate our results with human evaluation metrics. Human evaluation is time-consuming andmust be conducted carefully, in the same experimental setting, in order to ensure comparison across systems.We participated in DUC2006 in order to gain access to a human evaluation of our system, speci&#64257;cally theNIST content responsiveness score. In addition, a second human evaluation is coordinated by Columbia University, the Pyramid method (Nenkova et al., 2004). The Pyramid method requires two steps: &#64257;rst, a set ofsemantic equivalence classes are built for the sets of model summaries, with higher scores assigned to contentrepresented in multiple model summaries, and second, a person identi&#64257;es the content units in a peer summarythat are found in the set of semantic equivalence classes.3 The scores reported measure the amount of contentoverlap between the peer summary and the four model summaries.In this paper, we describe the multi-document summarization system we submitted to DUC2006. Our contribution in DUC2006, identi&#64257;ed as System 10, builds on an earlier system, SumBasic (Nenkova &amp; Vanderwende, 2005) which produces generic multi-document summaries; we provide a description of SumBasic inSection 2. We then describe each of the remaining three main components that comprise our system: atask-focused extractive summarization system, sentence simpli&#64257;cation, and lexical expansion of topic words.We will provide experiments using automatic metrics designed to quantify the contributions of each component. Human evaluation metrics, discussed in Section 6.2, indicate that this is a relatively successful approachto multi-document summarization; in the Pyramid evaluation, our system ranked &#64257;rst out of 22 systems and inthe NIST metrics for content responsiveness, our system ranked third out of 35 systems.With regard to our system design, it must be noted that this system, similar to almost all multi-documentsummarization systems, produces summaries by selecting sentences from the document set, either verbatim orwith some simpli&#64257;cation. Using sentence simpli&#64257;cation is a step towards generating new summary text, ratherthan extracting summaries from existing text. There is, however, no consideration for sentence ordering orcohesion other than that sentence ordering is determined exclusively as a result of the sentence selection process (see Section 2 for details).2. Core system: SumBasicSumBasic (Nenkova &amp; Vanderwende, 2005) is a system that produces generic multi-document summaries.Its design is motivated by the observation that words occurring frequently in the document cluster occur withhigher probability in the human summaries than words occurring less frequently. Speci&#64257;cally, SumBasic usesthe following algorithm:3In order to have summaries evaluated according to the Pyramid method, each participant was required to volunteer annotation e&#64256;ort,which we did. In order to further contribute to the community e&#64256;ort, Microsoft Research assisted in the creation of the semanticequivalence classes. Creation of the semantic equivalence tasks, which requires the model summaries, was carried out after &#64257;nal test resultshad been submitted, and therefore, we did not have access to the model summaries at any time prior to the test.1608L. Vanderwende et al. / Information Processing and Management 43 (2007) 1606&#8211;1618Step 1. Compute the probability distribution over the words wi appearing in the input, p(wi) for every i;p&#240;wi &#222; &#188; Nn , where n is the number of times the word appeared in the input, and N is the total number of contentword tokens in the input.Step 2. For each sentence Sj in the input, assign a weight equal to the average probability of the words in thesentence, i.e.,Weight&#240;S j &#222; &#188;XWi2Sjp&#240;wi&#222;:jfwijwi 2 SjgjStep 3. Pick the best scoring sentence that contains the highest probability word.Step 4. For each word wi in the sentence chosen at step 3, update their probability:pnew &#240;wi &#222; &#188; pold &#240;wi &#222;  pold &#240;wi &#222;:Step 5. If the desired summary length has not been reached, go back to Step 2.Steps 2 and 3 enforce the desired properties of the summarizer, i.e., that high frequency words from theinput are very likely to appear in the human summaries. Step 3 ensures that the highest probability word isincluded in the summary, thus each time a sentence is picked, the word with the highest probability at thatpoint in the summary is also picked. Step 4 serves a threefold purpose:1. It gives the summarizer sensitivity to context. The notion of &#8216;&#8216;what is most important to include in the summary?&#8217;&#8217; changes depending on what information has already been included in the summary. In fact, whilepold(wi) can be considered as the probability with which the word wi will be included in the summary,pnew(wi) is an approximation of the probability that the word wi will appear in the summary twice.2. By updating the probabilities in this intuitive way, we also allow words with initially low probability to havehigher impact on the choice of subsequent sentences.3. The update or word probability gives a natural way to deal with the redundancy in the multidocumentinput. No further checks for duplication seem to be necessary.The system resembles SUMavr as recently described in Nenkova, Vanderwende, and McKeown (2006)except that the update function in SumBasic uses squaring rather than multiplication by a very small number.Comparing SumBasic to Luhn (1958). The idea that simple frequency is indicative of importance in automatic summarization dates back to the seminal work of Luhn (1958). The SumBasic algorithm is distinct fromLuhn&#8217;s algorithm, however, in several signi&#64257;cant but illustrative ways. Luhn &#64257;rst collects the frequencies ofwords in the document and identi&#64257;es a subset of signi&#64257;cant words, excluding the most frequent (what wouldnow be termed &#8216;&#8216;stopwords&#8217;&#8217;) and the least frequent (generally, words occurring less than four times). WhereasSumBasic uses true initial probabilities and computes the weight of a sentence as equal to the average probability of the words in a sentence, Luhn treats all signi&#64257;cant words as having equal weight and computes theweight of a sentence as a function of the concentration of signi&#64257;cant words in the sentence. This weight isobtained by de&#64257;ning windows as sequences of signi&#64257;cant and non-signi&#64257;cant words, such that there are nomore than four non-signi&#64257;cant words between any two signi&#64257;cant words in a window; the weight of a sentenceis equal to the weight of the highest scoring window, namely, the square root of the number of signi&#64257;cantwords in the window, divided by the total number of words in the window. Finally, since Luhn&#8217;s system isdesigned to summarize single-documents, there is little need to prevent redundancy, in sharp contrast to multidocument summarization, where the likelihood that several documents might convey highly similar, or evenidentical, important information necessitates mechanisms to avoid redundancy, a functionality that SumBasicprovides by updating the probability of the words on the basis of preceding selected sentences.3. SumFocusNenkova and Vanderwende (2005) document the impact of frequency on generic multi-document systems,but in the case of task-focused summarization, document frequency alone may not be predictive, since theL. Vanderwende et al. / Information Processing and Management 43 (2007) 1606&#8211;16181609topic may not be related to the most common information in the document set. Thus, in order to incorporatetopic constraints in generic summarization, our new system, called SumFocus, captures the information conveyed by the topic description by computing the word probabilities of the topic description. Having done so,the weight for each word is computed as a linear combination of the unigram probabilities derived from thetopic description, with backo&#64256; smoothing to assign words not appearing in the topic a very small probability,and the unigram probabilities from the document, in the following manner (all other aspects of SumBasicremain unchanged):WordWeight &#188; &#240;1  k&#222;  DocWeight &#254; k  TopicWeight:The optimal value of k, 0.9, was empirically determined using the DUC2005 corpus, manually optimizing onROUGE-2 scores (henceforth R-2).Since sentence selection is controlled by choosing the words with the highest weights, it is, in principle,possible for these &#8216;&#8216;best words&#8217;&#8217; to come from either the document or from the topic description. In practice,however, the best word is nearly always a word from the topic description due to the high value assigned to k.For DUC2005 overall, 618 document words were identi&#64257;ed as best on the basis of topic statements and only22 independently on the basis of frequency alone. For DUC2006 overall, all 600 best words were chosen fromthe topic statements. On the basis of DUC2005 data, we added a small list of &#8216;&#8216;topic stopwords&#8217;&#8217; (describe,discuss, explain, identify, include, including, involve, involving) that did not receive any weight. The topicstatements in DUC2006 appear to contain more instructions to the summarizer than in DUC2005, suggestingadditional words may warrant similar handling (e.g., concerning, note, specify, give, examples, and involved).4. Sentence simpli&#64257;cationOur goal is to create a summarization system that produces summaries with as much content as possiblethat satis&#64257;es the user, given a set limit on length.4 Since summaries produced by SumFocus alone are extractive, we view sentence simpli&#64257;cation (also known as sentence shortening or sentence compression) as a means ofcreating more space within which to capture important content.4.1. Approaches to simpli&#64257;cationThe most common approach to sentence simpli&#64257;cation for summarization purposes has been to deterministically shorten the sentences selected to be used in the summary. The CLASSY system (Conroy, Schlesinger,&amp; Goldstein Stewart, 2005) for example, incorporates a heuristic component for sentence simpli&#64257;cation thatpreprocesses the sentences used in their sentence selection component. Columbia University&#8217;s summarizationsystem uses a syntactic simpli&#64257;cation component (Siddharthan, Nenkova, &amp; McKeown, 2004) the results ofwhich are sent to their sentence clustering component. Daume&#769; and Marcu (2005a) employ a post-processingapproach and report that deleting adverbs and attributive phrases improve ROUGE scores in the MultilingualSummarization Evaluation, although this post-processing was not found to be useful in DUC2005 (Daume&#769; &amp;Marcu, 2005b) conceivably because the summaries are 250 words long rather than 100 words long.In these approaches, simpli&#64257;cation operations apply to all sentences equally, and the core sentence selectioncomponent has only either the original or the shortened sentence available to choose from. This may not beoptimal, however, because the best simpli&#64257;cation strategy is not necessarily the same for all sentences. Forexample, it might be desirable to delete material X from a sentence only if X is already covered by anothersentence in the summary. For this reason, simpli&#64257;cation strategies have so far remained conservative, presumably to avoid possible side-e&#64256;ects of oversimpli&#64257;cation.An alternative approach to sentence simpli&#64257;cation is to provide multiple shortened sentence candidates forthe summarization engine to choose from. Multi-Document Trimmer (Zajic, Dorr, Lin, Monz, &amp; Schwartz,2005) for instance uses a syntactic simpli&#64257;cation engine (Dorr, Zajic, &amp; Schwartz, 2003) initially developedfor headline generation, to output multiple simpli&#64257;ed versions of the sentences in the document cluster. Each4For DUC2005 and DUC2006, the summary length was no more than 250 words.1610L. Vanderwende et al. / Information Processing and Management 43 (2007) 1606&#8211;1618Table 1Syntactic patterns for sentence simpli&#64257;cation (underlined parts are removed)PatternExampleNoun appositiveGerundive clauseOne senior, Liz Parker, had slacked o&#64256; too badly to graduateThe Kialegees, numbering about 450, are a landless tribe, sharing space in Wetumka, Okla., with the muchlarger Creek Nation, to whom they are relatedNonrestrictive relativeclauseThe return to whaling will be a sort of homecoming for the Makah, whose real name which cannot bewritten in English means &#8216;&#8216;people who live by the rocks and the seagulls&#8217;&#8217;Intra-sentential attributionLead adverbials andconjunctionsSeparately, the report said that the murder rate by Indians in 1996 was 4 per 100,000, below the nationalaverage of 7.9 per 100,000, and less than the white rate of 4.9 per 100,000of these candidates are submitted to the feature-based sentence selection component, which includes theredundancy score of the sentence given the current state of the summary and the number of trimming operations as features.4.2. Simpli&#64257;ed sentences as alternativesOur approach to sentence simpli&#64257;cation most closely resembles that used in the Multi-Document Trimmer(Zajic et al., 2005): we apply a small set of heuristics to a parse tree to create alternatives, after which both theoriginal sentence and (possibly multiple) simpli&#64257;ed versions are available for selection. Unlike MDT, however,in our system both original and alternative simpli&#64257;ed sentences are provided for selection without di&#64256;erentiation, i.e., without retaining any explicit link between them, under the assumption that the SumBasic-basedmulti-document summarization engine is inherently equipped with the ability to handle redundancy, andthe simpli&#64257;ed alternatives only add to that redundancy. SumBasic&#8217;s method for updating the unigram probabilities given the sentences already selected allows the simpli&#64257;ed sentence alternatives to be considered independently, while maintaining redundancy at a minimum.5 Given that this approach to sentence simpli&#64257;cationallows the sentence selection component to make the optimal decision among alternatives, we are thus freed topursue more aggressive simpli&#64257;cation, since the original non-simpli&#64257;ed version is always available for selection. The approach is also extensible to incorporating novel sentence rewrites into a summary, moving inthe direction of generative rather than extractive summaries. An example of this is Jing and McKeown(2000) who propose a set of operations to edit extracted sentences, not limited to sentence reduction. It isstraightforward to integrate such sentence rewrite operations into our framework, as candidate generationworks independently of sentence selection, and word probability alone su&#64259;ces to compute the sentence score.4.3. The syntax-based simpli&#64257;cation &#64257;lterOur simpli&#64257;cation component consists of heuristic templates for the elimination of syntactic units based onparser output. Each sentence in the document cluster is &#64257;rst parsed using a broad-coverage English parser(Ringger, Moore, Charniak, Vanderwende, &amp; Suzuki, 2004). We then run a &#64257;lter on the parse tree that eliminates certain nodes from the parse tree when the node matches the patterns provided heuristically. Table 1lists the syntactic patterns used in our DUC2006 system. These patterns are inspired by and similar to thosediscussed in Dunlavy et al. (2003) the principal di&#64256;erence being that extraction makes use of a full-&#64258;edged syntactic parser rather than employing a shallow parsing approach. For the &#64257;rst three patterns in Table 1 (nounappositive, gerundive clause and non-restrictive relative clause), the parser returns a node label correspondingexactly to these patterns; we simply deleted the nodes with these labels. For the identi&#64257;cation of intra-sentential attribution, we added speci&#64257;c conditions for detecting the verbs of attribution (said in Table 1), its subject(the report), the complementizer (that) and adverbial expressions if any, and deleted the nodes when conditionswere matched. In the case of sentence-initial adverbials, we delete only manner and time adverb expressions,5Note, however, that the probability update by SumBasic must be computed based on the original document cluster.L. Vanderwende et al. / Information Processing and Management 43 (2007) 1606&#8211;16181611using the features returned by the parser. Currently, these patterns all apply simultaneously to create maximally one simpli&#64257;ed sentence per input, but it would in principle be possible to generate multiple simpli&#64257;edcandidates. Finally, the punctuation and capitalization of the simpli&#64257;ed sentences are cleaned up before thesentences are made available to the selection component along with their original, non-simpli&#64257;ed counterpartsin the document cluster. The results of applying this simpli&#64257;cation &#64257;lter are discussed in Section 6.5. Lexical expansionIn addition to sentence simpli&#64257;cation, we also investigated the potential for expanding the task terms withsynonyms and morphologically-related forms. The use of query expansion has frequently been explored inInformation Retrieval tasks, but without notable success (Mitra, Singhal, &amp; Buckley, 1998). However, sincethe purpose of summarization is not to extract entire relevant documents from a large data set, but smallersentences, it might be hypothesized that individual expansions could have more evident impact. In constructing our system, therefore, we augmented the task terms with lexical expansions supplied by morphologicalvariants (chie&#64258;y derived forms) and synonyms or closely-related terms drawn from both static hand-craftedthesauri and dynamically-learned sources extracted from corpora.Lexical expansions are applied only at the point where we choose the inventory of &#8216;&#8216;best words&#8217;&#8217; with whichto determine candidate sentence selection. As already noted in Section 3, in DUC2006, the &#8216;&#8216;best words&#8217;&#8217; aredrawn only from the topic statements, and not from the document collection. Where a term in the sentencematches a lexical expansion, the formula for computing &#8216;&#8216;best word&#8217;&#8217; scores is as follows, where d is the document score of the lexical item in question, and e is the score for each type of matched expansion:nXExpansion Score &#188; &#240;1  k&#222;  d &#254; kei :PniThe summation i ei is the expanded analog of the TopicWeight in the formula given for unexpanded topics inthe discussion of SumFocus in Section 3. The k is a uniform default weight applied to the cumulative scores ofmatches from all expansion types. For the purposes of our DUC 2006 submission, the value of k was set at 0.5after hand inspecting results for R-2 while tuning the system on the DUC 2005 dataset. Lexical expansionscores were not used to recompute the lexical probabilities of the sentences once they had been selected.5.1. Morphological variantsMorphologically-derived forms were looked up in the version of the American Heritage Dictionary used byour parser, thereby allowing us to obtain pairs such as develop M development. We also employed a smallinventory of paired geographical names and their adjective counterparts, e.g., United States M American,China M Chinese, Tanzania M Tanzanian. Expansion scores for these morphological variants were computedon the basis of simple occurrence counts of the number of times the forms were encountered in the taskdescription. Thus, if both forms appeared in the topic text, both received a boost. Since our implementationdid not lemmatize the topic words, only exact matches were considered.5.2. Learned thesaurusA primary objective in investigating lexical expansions was to explore the potential impact of a 65,335-pairsynonym list that we automatically acquired from clustered news articles available on the World Wide Web,the hypothesis here being that a thesaurus derived from news data might be prove more useful, and potentiallydomain-relevant than static general-domain synonym resources. Starting with an initial dataset of 9.5 millionsentences in 32,400 pre-clustered news articles, we created a monolingual bitext of 282,600 aligned sentencepairs using a crude heuristic similarity measure based on three criteria, namely, that the sentence pairs shouldhave at least three words in common, have a minimum sentence length ratio of 66.6%, and have a word-basededit distance (i.e,, number of words inserted and deleted) of e 6 12. The reader is referred to Quirk, Brockett,and Dolan (2004) Dolan, Quirk, and Brockett (2004) for further information about the construction of thiscorpus. This was augmented by a further 20,000 sentence pairs extracted using a Support Vector Machine1612L. Vanderwende et al. / Information Processing and Management 43 (2007) 1606&#8211;1618Table 2Sample aligned sentence pair used to extract word associationsIndonesia&#8217;s electoral commission has formally announced that Susilo Bambang Yudhoyono, a former general and security minister, haswon the country&#8217;s &#64257;rst direct presidential electionIndonesian election o&#64259;cials today formally declared ex-general Susilo Bambang Yudhoyono as victor in the country&#8217;s &#64257;rst presidentialpoll after a &#64257;nal vote tally was completedapplying lexical bootstrapping methods described in Brockett and Dolan (2005) who demonstrate that it ispossible to apply Giza++ (Och &amp; Ney, 2003) to this (more or less comparable) corpus to yield an overall wordAlignment Error Rate as low as 12.46%. Sentence pairs obtained in this fashion typically exhibit substantivesemantic overlap, providing a dataset from which a potentially useful thesaurus might be extracted. Anexample sentence pair is shown in Table 2.The paired sentences were then tokenized, named entities and multiple word expressions were identi&#64257;ed,and the words then lemmatized and tagged for part of speech. Identical words were deleted from the pairsand the remainder aligned using a Log Likelihood Ratio-based Word Association technique described inMoore (2001) using the formula given in Moore (2004) modi&#64257;ed here for readability:X Xp&#240;tjs&#222;LLR&#240;t; s&#222; &#188;;C&#240;t; s&#222; logp&#240;t&#222;t2f1;0g s2f1;0gwhere t and s are variables ranging over the presence (1) or absence (0) of the two words under consideration,and C(t, s) is the observed joint count for their values. The probabilities used are maximum likelihoodestimates.After extraction, the word pairs were further &#64257;ltered to remove typographical errors, mismatches relating tonumerical expressions and other artifacts of unnormalized news data. The pairs were then chained up to threesteps to expand the dataset, and the association scores coerced into a range between 1.0 and 0.0 in order togenerate a distribution that could be utilized by the system. These initial weights were then updated at runtimein the same manner as for morphological variants, by multiplying them by the number of times either memberof a pair occurred in the topic description. No attempt was made to discriminate among word senses, in particular, where chaining might have resulted in mismatched terms. (In general, such mismatches would tend tohave very low scores.) Example synonyms and near synonyms for the word &#8216;&#8216;virus&#8217;&#8217; extracted in this mannerare shown in Table 3. These range over two major senses, and also are not limited to the same part of speech.On the other hand, the data acquired in this manner can be relatively limited, and does not include, for example, names of major computer viruses, a limitation that in fact appears to have a&#64256;ected results in somenewsclusters.5.3. Static thesauriIn addition to the learned synonyms, we also considered expansions using data extracted from two staticthesauri. The &#64257;rst was a list of 125,054 word pairs in the Encarta Thesaurus (Rooney, 2001) which we deployedTable 3Synonyms of the noun &#8216;&#8216;virus&#8217;&#8217; learned from monolingual parallel corpora with initially assigned weightsSynonymWeightSynonymWeightCoronavirusDiseaseComputer_virusWormIllnessCancerEpidemicViralFlu0.1732060.1134150.1056000.0949180.0519180.0389960.0387780.0233260.020093Heart_diseaseSevereInfectSARSWest_Nile_VirusConditionDisease-causingHIV0.0188870.0182780.0143610.0138730.0064700.0063350.0044640.001398L. Vanderwende et al. / Information Processing and Management 43 (2007) 1606&#8211;16181613in the submitted system. Heuristic weights were precomputed in the form of a distribution based on number ofsynonyms for each word, allowing this data to be easily integrated with our acquired Word Association list.We also experimented with, but did not include in the system submitted to DUC 2006, simple undisambiguated synonym expansion using WordNet 2.0 (Fellbaum, 1998) No attempt was made to provide weights forthe lexical expansions found in WordNet; instead raw occurrence counts were used as with morphologicalvariants, with the uniform k applied in the same manner as to the other resources.6. Results6.1. Automatic evaluation using ROUGETable 4 compares the average ROUGE recall scores for word unigram (R-1), word bigram (R-2) and skip-4bigram (R-SU4) models achieved on DUC 2005 data (used in training) and on DUC 2006 data by di&#64256;erentversions of our system.6 Numbers for SumBasic are presented as baseline.It is di&#64259;cult to derive meaningful conclusions on the basis of the ROUGE results in Table 4. On DUC 2005data, sentence simpli&#64257;cation consistently appears to improve matters modestly, but on DUC 2006 data it paradoxically seems to introduce a small degradation in recall when deployed in conjunction with SumFocus.This appears to be partially o&#64256;set by the application of lexical expansion. However, the di&#64256;erences in scoresconsistently fall within the 95% error margins computed by the ROUGE tool. Moreover, p-scores computedusing the Wilcoxon Matched-Pairs Signed-Ranks Test indicate that di&#64256;erences over a baseline Sumbasic system without simpli&#64257;cation were signi&#64257;cant (p &lt; 0.05) only on the DUC 2005 data used in training.Despite the somewhat inconclusive nature of the ROUGE results, they are suggestive of the relative contributions of di&#64256;erent system components. Table 5 presents the individual contributions made by variousexpansion strategies when used in conjunction with sentence simpli&#64257;cation. The biggest observable impactcomes from morphological expansion (MRF), while other lexical expansions, namely the Encarta Thesaurus(ENC) and Word Association data (WA) may have contributed to the overall performance of the system to alesser extent, if at all. MRF + ENC + WA corresponds to the system submitted to DUC 2006, i.e., SumFocuswith lexical expansion and simpli&#64257;cation. None of the di&#64256;erences are statistically signi&#64257;cant at the level ofp &lt; 0.05 on the Wilcoxon Matched-Pairs Signed Ranks Test. Table 5 also shows the potential impact of simplesynonym expansion using WordNet 2.0 (Fellbaum, 1998) which was not included in the submitted system, butwhich performs at or below baseline level in the table.6.2. Pyramid and NISTResults obtained from human evaluations are potentially more diagnostic and can inform the direction thatfuture work should take. Unfortunately, however, NIST or Pyramid, as one-time human evaluations, do notlend themselves to comparing system settings. Nevertheless, since both NIST and Pyramid evaluation techniques measure content directly, these are the metrics we focus on, given our primary goal of maximizing summary content. Accordingly, we report NIST and Pyramid metrics only for the system we submitted, which isSumFocus with sentence simpli&#64257;cation and with lexical expansion of the topic words.Pyramid evaluation. System 10 ranked &#64257;rst in the overall mean Pyramid score of the 22 systems that participated in the Pyramid evaluation. It must be noted, however, that the maximum Pyramid score for eachcluster di&#64256;ers (Nenkova et al., 2004) since the agreement observed in the model summaries varies from clusterto cluster, and so we &#64257;nd that the average rank is a better method to compare systems, presented in Table 6.System 10 is ranked &#64257;rst for 5 out of 20 clusters, and is in the top 3 for half of the clusters. Overall, our percluster mean ranking (5.90) is the best among the 22 systems. On the other hand, our performance across the6DUC reports ROUGE numbers that are computed implementing the &#8216;&#8216;jackknife&#8217;&#8217; in which each human model summary is successivelyremoved from the evaluation and added to the system (&#8216;&#8216;peer&#8217;&#8217;) summaries. This permits the human summaries to be directly comparedwith the system summaries using the same metric. For all experiments in this paper, however, we compute ROUGE without jackkni&#64257;ng,which allows us to use 4 model summaries. As a result, the o&#64259;cial DUC 2006 scores vary slightly from the numbers reported here; thedi&#64256;erences, however, are not signi&#64257;cant.1614L. Vanderwende et al. / Information Processing and Management 43 (2007) 1606&#8211;1618Table 4ROUGE results, with and without sentence simpli&#64257;cation (not using stopwords)DUC 2005DUC 2006Not simpli&#64257;edSimpli&#64257;edp6R-1RecallSumBasicSumFocusSumFocus + expansion0.256050.253580.254890.260540.260110.26006R-2RecallSumBasicSumFocusSumFocus + expansion0.036420.040610.041040.036530.042670.042440.039R-SU4RecallSumBasicSumFocusSumFocus + expansion0.066310.069740.070490.067210.072320.072210.031Not simpli&#64257;edSimpli&#64257;edp60.300260.299950.300640.307530.296930.299430.053260.059000.058960.055090.057250.059490.0550.085720.091070.090930.086810.088520.089980.125Table 5Contributions of di&#64256;erent lexical expansion components on DUC2006 data (with sim-pli&#64257;cation, not using stopwords)SumFocus (Baseline)ENCWNWAMRFMRF + ENC + WAR-1R-2R-SU40.296930.296420.295750.296780.298620.299430.057250.057190.057290.057360.059180.059490.088520.088410.088340.088490.089790.08998ENC: Encarta Thesaurus, MRF: morphological variants, WA: word Association, WN: WordNet.Table 6Pyramid and NIST results for system 10, including the rank per cluster according to Pyramid evaluation, average number of unweightedSCUs out of the average number of SCUs attainable for each cluster, and the NIST content responsiveness score, which is on a scale of 5to 1ClusterRank acc. toPyramid scoreSystem 10 SCUs/averageSCUs attainableNIST content responsiveness(5 = very good)D0601D0603D0605D0608D0614D0615D0616D0617D0620D0624D0627D0628D0629D0630D0631D0640D0643D0645D0647D0650126110131211515211311268630.30560.18180.09760.32000.17390.25000.30430.34090.31030.50000.14290.16130.04350.19350.56250.25640.30770.17860.16000.275013235142331123422322clusters, also shown in Table 6, is not evenly distributed; system 10 scores worse than half of the systems for 5out of 20 clusters. Table 6 also presents the NIST content responsiveness score, i.e., the human judgment ofL. Vanderwende et al. / Information Processing and Management 43 (2007) 1606&#8211;16181615Table 7NIST evaluation resultsGrammaticalityNon-redundancyReferential clarityResponsivenessSystem 10Avg. peerSystem 10 rank3.124.422.642.943.584.233.112.543110313how well the content responds to the needs expressed in the topic description, on a scale from 1 to 5, where 5 isthe highest achievable score. Overall, System 10 ranked third on responsiveness. On a per-cluster basis, providing a system ranking according the NIST content score is less than instructive since there are only 5 valuesthat can be given. Initial investigation showed there is no correlation between Pyramid score and NIST content score, but we have reached no conclusion yet. It is unexpected that for cluster D0601, system 10 is rankedhighest among the peers in the Pyramid evaluation, with a relatively high degree of SCU overlap, while receiving a poor NIST content score for the same cluster.Looking at the Pyramid analysis a bit more deeply, we also computed the percentage of the number ofSCUs that were scored in System 10 and the number of SCUs that could be attained within the 250 word limit.We note that for only one cluster are more than half of the attainable SCUs found in System 10&#8217;s peer summaries. Improving this percentage is a clear direction for future research, though it remains an open questionwhether improving this percentage will lead to higher content responsiveness scores.NIST evaluation. Three of the &#64257;ve NIST linguistic quality questions are relevant to sentence simplifcation:grammaticality, non-redundancy and referential clarity, along with the responsiveness question. NIST evaluated these questions by eliciting human judgments on a &#64257;ve point scale. Table 7 shows our DUC2006 scoresrelative to the peers, along with the rank of our system among 35 peer systems.Though our system (System 10) implements only a small number of simpli&#64257;cation patterns, as described inSection 4, its e&#64256;ect was quite extensive. In DUC2006, of all the sentences selected in summary, 43.4% of themwere the result of simpli&#64257;cation. This resulted in adding on average another sentence to the summary: theaverage number of sentence in a summary increased from 11.32 to 12.52 when we used sentence simpli&#64257;cation.7. Discussion7.1. Contribution of simpli&#64257;cationThe Pyramid results suggest that making room for more content and removing redundant material bysimplifying sentences is a promising operation for extractive summarization systems. It is also interesting tonote that 33.6% of the sentences selected in the summaries were original non-simpli&#64257;ed versions, even thougha simpli&#64257;ed counterpart was also available. Manual investigation of the summary for one cluster (D0631D)established that four out of eleven sentences were non-simpli&#64257;ed despite the availability of a simpli&#64257;ed alternative. These four sentences are shown in Table 8. Of these, two were incorrectly parsed as noun appositives,resulting in an unexpectedly large portion of the text being deleted and rendering the simpli&#64257;ed version lesslikely to be selected. The other two sentences present interesting cases where the deleted portion of text (indicated in boldface in Table 8) included important content, corresponding to Summary Content Units (SCUs)observed in three or four of the model summaries according to the Pyramid evaluation. This manual examination con&#64257;rms that the best simpli&#64257;cation strategy may not be the same in all cases, and that there arebene&#64257;ts in using the summarizer itself to choose the best sentence alternative given the context.Sentence simpli&#64257;cation undoubtedly contributes to our low score for grammaticality. In part this may bebecause the parser-based simpli&#64257;cation &#64257;lter can produce ungrammatical sentences and sentences withdegraded readability owing to misplaced punctuation marks. However, grammaticality judgments may alsobe a&#64256;ected by whether or not a system allows incomplete last sentences. Since the length limit in DUC2006was 250 words, some systems kept the summaries below 250 words, but permitted only complete sentences,while other systems truncated the summary at 250 words, even if the last sentence was incomplete. SumFocus1616L. Vanderwende et al. / Information Processing and Management 43 (2007) 1606&#8211;1618Table 8Examples of full sentences chosen instead of their simpli&#64257;ed counterpartReason: parser errorLondon British aviation authorities on Wednesday formally ruled the Concorde supersonic airliner un&#64257;t to &#64258;y unless its manufacturerstook steps to prevent the problems that led to last month&#8217;s fatal Air France Concorde crash near ParisLe Figaro newspaper on Wednesday quoted Gayssot, the transport minister, as raising the possibility that the ban on Air FranceConcorde &#64258;ights could remain in place until the Accident and Inquiry O&#64259;ce releases a preliminary report on the crash at the end ofAugustReason: deleted material contains important informationParis French investigators looking into the crash last month of an Air France Concorde said Thursday it was probable that a 16-in. pieceof metal found on the runway caused a tire to blow out, sending debris from the tire through fuel tanks and triggering a &#64257;re thatbrought down the planeThe sleek, needle-nosed aircraft could cross the Atlantic at an altitude of 60,000 feet and at 1350 mph, completing the trip from London toNew York in less than four hours &#8211; half the time of regular jetsUnderlined portion of text was deleted in the simpli&#64257;ed sentence. Text corresponding to an SCU is indicated by boldface.falls into this latter category. Only 2 systems allowing incomplete last sentences scored higher on grammaticality than those with complete last sentences, suggesting that while the low scores for grammaticality may bedue to sentence simpli&#64257;cation, they may also be a function of system design.Referential quality may also be negatively a&#64256;ected by the current simpli&#64257;cation &#64257;lter, since deletion of intrasentential attribution can result in deleting the antecedent of pronouns in the summary. On the other hand, itis encouraging to note that our methods perform well on non-redundancy and content responsiveness. In particular, providing sentence alternatives did not increase redundancy, even though the alternatives were notexplicitly linked, indicating that the method of updating unigram weights given context used in SumFocusis robust enough to handle the greater redundancy introduced by providing simpli&#64257;ed alternatives.7.2. Contribution of lexical expansionIt is di&#64259;cult to argue on the basis of either human or automated evaluation that lexical expansion significantly enhanced the performance of SumFocus, even when using simpli&#64257;cation. We earlier noted the paucityof evidence that thesaurus-based query expansion (as opposed to adding search terms) signi&#64257;cantly improvesthe relevance of IR search results: Voorhees (1988) has suggested that use of WordNet may improve results ofshort queries, but not of longer ones. Since the DUC2006 tasks constitute moderately extended queries, it isperhaps unsurprising that the expansions had minimal e&#64256;ect. It is likely also that system-related factors mitigated the impact of lexical expansions: for example, it is probable that the means by which the expansionscores were computed may have resulted in many word pairs being scored too low to have any e&#64256;ect, whilethe use of a uniform expansion k failed to di&#64256;erentiate di&#64256;erent expansion types adequately.Analysis of ROUGE scores for individual clusters indicates that depending on the domain and type of task,results were in fact sometimes negatively impacted by the approach taken in SumFocus as opposed to SumBasic. In DUC 2006, we observed low individual cluster scores when topic descriptions required the summaryto identify a class of items: for example, a description that contains the instruction &#8216;&#8216;Identify computer virusesdetected worldwide,&#8217;&#8217; is expected to yield sentences containing the names of speci&#64257;c computer viruses, such asMelissa, Love Bug and CIH, rather than multiple exemplars of the term computer virus. Here, it seems thateven the use of dynamically-learned word associations was inadequate to this domain-speci&#64257;c task. In the longterm, however, acquisition of better domain-relevant resources might be expected to ameliorate system performance in such cases.8. Conclusions and future workThe Pyramid annotation shows us that only rarely does a peer summary match 50% or more of the contentin the combined model summaries. A detailed analysis of the percentage of SCUs per weight must still beL. Vanderwende et al. / Information Processing and Management 43 (2007) 1606&#8211;16181617done, but anecdotally, our system matches only half of the high-scoring SCUs, i.e., those SCUs that werefound in all of the model summaries. Clearly, &#64257;nding methods to model this data more closely o&#64256;ers opportunities for improving the overall content of summaries. One direction will lead us to &#64257;nd more sophisticatedmethods of modeling the relation between a topic word and any of its lexical expansions, the document words,and the target summaries. And, as we continue to leave purely extractive summarization behind and pursuesummarization that generates novel sentences that better capture the information need of the user, we will alsoexpand our system to take full advantage of sentence simpli&#64257;cation component. In particular, we plan toinclude more drastic simplifying and rewriting operations (such as splitting coordinated clauses) and producemultiple candidates per sentence in order to see the full potential for the proposed approach.</corps><conclusion>s and future workThe Pyramid annotation shows us that only rarely does a peer summary match 50% or more of the contentin the combined model summaries. A detailed analysis of the percentage of SCUs per weight must still beL. Vanderwende et al. / Information Processing and Management 43 (2007) 1606&#8211;16181617done, but anecdotally, our system matches only half of the high-scoring SCUs, i.e., those SCUs that werefound in all of the model summaries. Clearly, &#64257;nding methods to model this data more closely o&#64256;ers opportunities for improving the overall content of summaries. One direction will lead us to &#64257;nd more sophisticatedmethods of modeling the relation between a topic word and any of its lexical expansions, the document words,and the target summaries. And, as we continue to leave purely extractive summarization behind and pursuesummarization that generates novel sentences that better capture the information need of the user, we will alsoexpand our system to take full advantage of sentence simpli&#64257;cation component. In particular, we plan toinclude more drastic simplifying and rewriting operations (such as splitting coordinated clauses) and producemultiple candidates per sentence in order to see the full potential for the proposed approach.</conclusion><discussion>We are indebted to Arul Menezes for implementing several features described here. We also thank theNIST team that supports DUC for the NIST evaluation, and we thank the DUC community who contributedto the Pyramid evaluation. We extend our appreciation to members of the Butler-Hill Group, especially BenGelbart, for their assistance with the Pyramid creation and annotation. Finally, we are grateful to the anonymous reviewers for their many constructive comments.</discussion><biblio>Brockett, C., &amp; Dolan, W. B. (2005). Support vector machines for paraphrase identi&#64257;cation and corpus construction. In Proceedings of the third international workshop on paraphrasing (IWP2005), Jeju, Republic of Korea. Conroy, J. M., Schlesinger, J., &amp; Goldstein Stewart, J. (2005). CLASSY query-based multi-document summarization. In Proceedings of DUC 2005.
 Daume&#769;, H., III, &amp; Marcu, D. (2005a). Bayesian multi-document summarization at MSE. In Proceedings of MSE 2005.
 Daume&#769;, H., III, &amp; Marcu, E. (2005b). Bayesian summarization at DUC and a suggestion for extrinsice evaluation. In Proceedings of DUC 2005.
 Dolan, W. B., Quirk, C., &amp; Brockett, C. (2004). Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In Proceedings of COLING 2004, Geneva, Switzerland. Dorr, B., Zajic, D., &amp; Schwartz, R. (2003). Hedge trimmer: a parse-and-trim approach to headline generation. In Proceedings of HLTNAACL 2003 text summarization workshop (pp. 1&#8211;8). Dunlavy, D., Conroy, J., Schlesinger, J., Goodman, S., Okurowski, M., O&#8217;Leary, E., et al. (2003). Performance of a three-stage system for multi-document summarization. In Proceedings of DUC 2003.
 Fellbaum, C. (Ed.). (1998). WordNet: An electronic lexical database. The MIT Press. Jing, H., &amp; McKeown, K. (2000). Cut and past based text summarization. In Proceedings of the 1st conference of the North American Chapter of the Association for Computational Linguistics (NAACL&#8217;00). Lin, C. Y. (2004). ROUGE: a package for automatic evaluation of summaries. In Proceedings of the workshop on text summarization branches out, 25&#8211;26 July 2004, Barcelona, Spain. Luhn, H. P. (1958). The automatic creation of literature abstracts. IBM Journal of Research and Development, 2(2), 159&#8211;165. Mitra, M., Singhal, A., &amp; Buckley, C. (1998). Improving automatic query expansion. In Proceedings of the 21st Annual International ACMSIGIR conference on research and development in information retrieval. Moore, R. C. (2001). Towards a simple and accurate statistical approach to learning translation relationships among words. In Proceedings, workshop on data-driven machine translation, Toulouse, France. Moore, R. C. (2004). Association-based bilingual word alignment. In Proceedings, workshop on building and using parallel texts: datadriven machine translation and beyond, Ann Arbor, MI. Nenkova, A., &amp; Vanderwende, L. (2005). The impact of frequency on summarization. MSR-TR-2005-101. Nenkova, A., Vanderwende, L., &amp; McKeown, K. (2006). A compositional context sensitive multidocument summarizer. In Proceedings of SIGIR 2006.
 Nenkova, A., &amp; Passonneau, R. (2004). Evaluating content selection in summarization: the Pyramid method. In Proceedings of the HLTNAACL 2004.
 Och, F. J., &amp; Ney, H. (2003). A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1), 19&#8211;52. Quirk, C., Brockett, C., &amp; Dolan, W. B. (2004). Monolingual machine translation for paraphrase generation. In Proceedings of the 2004 conference on empirical methods in natural language processing, 25&#8211;26 July 2004, Barcelona Spain (pp. 142&#8211;149). Ringger, E., Moore, R. C., Charniak, E., Vanderwende, L., &amp; Suzuki, H. (2004). Using the Penn treebank to evaluate non-treebank parsers. In Proceedings of LREC 2004.
 Rooney, K. (2001). Encarta Thesaurus. Bloomsbury Publishing.  1618  L. Vanderwende et al. / Information Processing and Management 43 (2007) 1606&#8211;1618  Siddharthan, A., Nenkova, A., &amp; McKeown, K. (2004). Syntactic simpli&#64257;cation for improving content selection in multi-document summarization. In Proceedings of COLING 2004.
 Voorhees, E. (1988). Using WordNet for text retrieval. In C. Fellbaum (Ed.), WordNet: An electronic lexical database. The MIT Press. Zajic, D., Dorr, B., Lin, J., Monz, C., &amp; Schwartz, R. (2005). A sentence-trimming approach to multi-document summarization. In Proceedings of DUC2005.
  </biblio></article>