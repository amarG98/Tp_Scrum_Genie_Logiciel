Nom du fichier d’origine : Furui 2004 Speech-to-text and speech-to-speech summarization of spontaneous speech



Titre du papier  : IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 12, NO. 4, JULY 2004

Auteur :  Speech-to-text and speech-to-speech summarization of spontaneous speech


Abstract de l’auteur : —This paper presents techniques for speech-to-textand speech-to-speech automatic summarization based on speechunit extraction and concatenation. For the former case, atwo-stage summarization method consisting of important sentenceextraction and word-based sentence compaction is investigated.Sentence and word units which maximize the weighted sum oflinguistic likelihood, amount of information, confidence measure,and grammatical likelihood of concatenated units are extractedfrom the speech recognition results and concatenated for producing summaries. For the latter case, sentences, words, andbetween-filler units are investigated as units to be extracted fromoriginal speech. These methods are applied to the summarizationof unrestricted-domain spontaneous presentations and evaluatedby objective and subjective measures. It was confirmed that proposed methods are effective in spontaneous speech summarization.
Introduction : 
O

NE OF THE KEY applications of automatic speech
recognition is to transcribe speech documents such as
talks, presentations, lectures, and broadcast news [1]. Although
speech is the most natural and effective method of communication between human beings, it is not easy to quickly review,
retrieve, and reuse speech documents if they are simply recorded
as audio signal. Therefore, transcribing speech is expected to
become a crucial capability for the coming IT era. Although
high recognition accuracy can be easily obtained for speech
read from a text, such as anchor speakers’ broadcast news
utterances, technological ability for recognizing spontaneous
speech is still limited [2]. Spontaneous speech is ill-formed
and very different from written text. Spontaneous speech
usually includes redundant information such as disfluencies,
fillers, repetitions, repairs, and word fragments. In addition,
irrelevant information included in a transcription caused by
recognition errors is usually inevitable. Therefore, an approach
in which all words are simply transcribed is not an effective
one for spontaneous speech. Instead, speech summarization
which extracts important information and removes redundant

Manuscript received May 6, 2003; revised December 11, 2003. The associate
editor coordinating the review of this manuscript and approving it for publication was Dr. Julia Hirschberg.
S. Furui, T. Kikuchi, and Y. Shinnaka are with the Department of Computer Science, Tokyo Institute of Technology, Tokyo, 152-8552, Japan
(e-mail: furui@furui.cs.titech.ac.jp; kikuchi@furui.cs.titech.ac.jp; shinnaka@
furui.cs.titech.ac.jp).
C. Hori is with the Intelligent Communication Laboratory, NTT
Communication Science Laboratories, Kyoto 619-0237, Japan (e-mail:
chiori@cslab.kecl.ntt.co.jp).
Digital Object Identifier 10.1109/TSA.2004.828699

and incorrect information is ideal for recognizing spontaneous
speech. Speech summarization is expected to save time for
reviewing speech documents and improve the efficiency of
document retrieval.
Summarization results can be presented by either text or
speech. The former method has advantages in that: 1) the
documents can be easily looked through; 2) the part of the documents that are interesting for users can be easily extracted; and
3) information extraction and retrieval techniques can be easily
applied to the documents. However, it has disadvantages in
that wrong information due to speech recognition errors cannot
be avoided and prosodic information such as the emotion of
speakers conveyed only in speech cannot be presented. On the
other hand, the latter method does not have such disadvantages
and it can preserve all the acoustic information included in the
original speech.
Methods for presenting summaries by speech can be classified into two categories: 1) presenting simply concatenated
speech segments that are extracted from original speech or
2) synthesizing summarization text by using a speech synthesizer. Since state-of-the-art speech synthesizers still cannot
produce completely natural speech, the former method can
easily produce better quality summarizations, and it does
not have the problem of synthesizing wrong messages due
to speech recognition errors. The major problem in using
extracted speech segments is how to avoid unnatural noisy
sound caused by the concatenation.
There has been much research in the area of summarizing
written language (see [3] for a comprehensive overview). So
far, however, very little attention has been given to the question
of how to create and evaluate spoken language summarization
based on automatically generated transcription from a speech
recognizer. One fundamental problem with the summaries produced is that they contain recognition errors and disfluencies.
Summarization of dialogues within limited domains has been
attempted within the context of the VERBMOBIL project [4].
Zechner and Waibel have investigated how the accuracy of the
summaries changes when methods for word error rate reduction
are applied in summarizing conversations in television shows
[5]. Recent work on spoken language summarization in unrestricted domains has focused almost exclusively on Broadcast
News [6], [7]. Koumpis and Renals have investigated the transcription and summarization of voice mail speech [8]. Most of
the previous research on spoken language summarization have
used relatively long units, such as sentences or speaker turns, as
minimal units for summarization.
This paper investigates automatic speech summarization
techniques with the two presentation methods in unrestricted

1063-6676/04$20.00 © 2004 IEEE

40
Corps : ONE OF THE KEY applications of automatic speechrecognition is to transcribe speech documents such astalks, presentations, lectures, and broadcast news [1]. Althoughspeech is the most natural and effective method of communication between human beings, it is not easy to quickly review,retrieve, and reuse speech documents if they are simply recordedas audio signal. Therefore, transcribing speech is expected tobecome a crucial capability for the coming IT era. Althoughhigh recognition accuracy can be easily obtained for speechread from a text, such as anchor speakers’ broadcast newsutterances, technological ability for recognizing spontaneousspeech is still limited [2]. Spontaneous speech is ill-formedand very different from written text. Spontaneous speechusually includes redundant information such as disfluencies,fillers, repetitions, repairs, and word fragments. In addition,irrelevant information included in a transcription caused byrecognition errors is usually inevitable. Therefore, an approachin which all words are simply transcribed is not an effectiveone for spontaneous speech. Instead, speech summarizationwhich extracts important information and removes redundantManuscript received May 6, 2003; revised December 11, 2003. The associateeditor coordinating the review of this manuscript and approving it for publication was Dr. Julia Hirschberg.S. Furui, T. Kikuchi, and Y. Shinnaka are with the Department of Computer Science, Tokyo Institute of Technology, Tokyo, 152-8552, Japan(e-mail: furui@furui.cs.titech.ac.jp; kikuchi@furui.cs.titech.ac.jp; shinnaka@furui.cs.titech.ac.jp).C. Hori is with the Intelligent Communication Laboratory, NTTCommunication Science Laboratories, Kyoto 619-0237, Japan (e-mail:chiori@cslab.kecl.ntt.co.jp).Digital Object Identifier 10.1109/TSA.2004.828699and incorrect information is ideal for recognizing spontaneousspeech. Speech summarization is expected to save time forreviewing speech documents and improve the efficiency ofdocument retrieval.Summarization results can be presented by either text orspeech. The former method has advantages in that: 1) thedocuments can be easily looked through; 2) the part of the documents that are interesting for users can be easily extracted; and3) information extraction and retrieval techniques can be easilyapplied to the documents. However, it has disadvantages inthat wrong information due to speech recognition errors cannotbe avoided and prosodic information such as the emotion ofspeakers conveyed only in speech cannot be presented. On theother hand, the latter method does not have such disadvantagesand it can preserve all the acoustic information included in theoriginal speech.Methods for presenting summaries by speech can be classified into two categories: 1) presenting simply concatenatedspeech segments that are extracted from original speech or2) synthesizing summarization text by using a speech synthesizer. Since state-of-the-art speech synthesizers still cannotproduce completely natural speech, the former method caneasily produce better quality summarizations, and it doesnot have the problem of synthesizing wrong messages dueto speech recognition errors. The major problem in usingextracted speech segments is how to avoid unnatural noisysound caused by the concatenation.There has been much research in the area of summarizingwritten language (see [3] for a comprehensive overview). Sofar, however, very little attention has been given to the questionof how to create and evaluate spoken language summarizationbased on automatically generated transcription from a speechrecognizer. One fundamental problem with the summaries produced is that they contain recognition errors and disfluencies.Summarization of dialogues within limited domains has beenattempted within the context of the VERBMOBIL project [4].Zechner and Waibel have investigated how the accuracy of thesummaries changes when methods for word error rate reductionare applied in summarizing conversations in television shows[5]. Recent work on spoken language summarization in unrestricted domains has focused almost exclusively on BroadcastNews [6], [7]. Koumpis and Renals have investigated the transcription and summarization of voice mail speech [8]. Most ofthe previous research on spoken language summarization haveused relatively long units, such as sentences or speaker turns, asminimal units for summarization.This paper investigates automatic speech summarizationtechniques with the two presentation methods in unrestricted1063-6676/04$20.00 © 2004 IEEE402IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 12, NO. 4, JULY 2004domains. In both cases, the most appropriate sentences, phrasesor word units/segments are automatically extracted from original speech and concatenated to produce a summary under theconstraint that extracted units cannot be reordered or replaced.Only when the summary is presented by text, transcriptionis modified into a written editorial article style by certainrules. When the summary is presented by speech, a waveformconcatenation-based method is used.Although prosodic features such as accent and intonationcould be used for selection of important parts, reliable methodsfor automatic and correct extraction of prosodic features fromspontaneous speech and for modeling them have not yet beenestablished. Therefore, in this paper, input speech is automatically recognized and important segments are extracted basedonly on the textual information.Evaluation experiments are performed using spontaneouspresentation utterances in the Corpus of Spontaneous Japanese(CSJ) made by the Spontaneous Speech Corpus and ProcessingProject [9]. The project began in 1999 and is being conductedover a five-year period with the following three major targets.1) Building a large-scale spontaneous speech corpus (CSJ)consisting of roughly 7 M words with a total speechlength of 700 h. This mainly records monologues suchas lectures, presentations and news commentaries. Therecordings with low spontaneity, such as those from readtext, are excluded from the corpus. The utterances aremanually transcribed orthographically and phonetically.One-tenth of them, called Core, are tagged manuallyand used for training a morphological analysis andpart-of-speech (POS) tagging program for automatically analyzing all of the 700-h utterances. The Core isalso tagged with para-linguistic information includingintonation.2) Acoustic and language modeling for spontaneous speechunderstanding using linguistic, as well as para-linguistic,information in speech.3) Investigating spontaneous speech summarization technology.II. SUMMARIZATION WITH TEXT PRESENTATIONA. Two-Stage Summarization MethodFig. 1 shows the two-stage summarization method consistingof important sentence extraction and sentence compaction [10].Using speech recognition results, the score for important sentence extraction is calculated for each sentence. After removingall the fillers, a set of relatively important sentences is extracted,and sentence compaction using our proposed method [11], [12]is applied to the set of extracted sentences. The ratio of sentenceextraction and compaction is controlled according to a summarization ratio initially determined by the user.Speech summarization has a number of significant challenges that distinguish it from general text summarization.Applying text-based technologies to speech is not alwaysworkable and often they are not equipped to capture speechspecific phenomena. Speech contains a number of spontaneouseffects, which are not present in written language, such ashesitations, false starts, and fillers. Speech is, to some extent,Fig. 1. A two-stage automatic speech summarization system with textpresentation.always distorted by ungrammatical and various redundantexpressions. Speech is also a continuous phenomenon thatcomes without unambiguous sentence boundaries. In addition,errors in transcriptions of automatic speech recognition enginescan be quite substantial.Sentence extraction methods on which most of the textsummarization methods [13] are based cannot cope with theproblems of distorted information and redundant expressionsin speech. Although several sentence compression methodshave also been investigated in text summarization [14], [15],they rely on discourse and grammatical structures of the inputtext. Therefore, it is difficult to apply them to spontaneousspeech with ill-formed structures. The method proposed in thispaper is suitable for applying to ill-formed speech recognitionresults, since it simultaneously uses various statistical features,including a confidence measure of speech recognition results.The principle of the speech-to-text summarization method isalso used in the speech-to-speech summarization which will bedescribed in the next section. Speech-to-speech summarizationis a comparatively much younger discipline, and has not yetbeen investigated in the same framework as the speech-to-textsummarization.1) Important Sentence Extraction: Important sentence extraction is performed according to the following score for each, obtained as a result of speechsentencerecognition(1)where is the number of words in the sentenceand,, andare the linguistic score, the significance, respectively.score, and the confidence score of wordAlthough sentence boundaries can be estimated using linguisticand prosodic information [16], they are manually given in theexperiments in this paper. The three scores are a subset ofthe scores originally used in our sentence compaction methodand considered to be useful also as measures indicating theFURUI et al.: SPEECH-TO-TEXT AND SPEECH-TO-SPEECH SUMMARIZATIONappropriateness of including the sentence in the summary.andare weighting factors for balancing the scores. Detailsof the scores are as follows.Linguistic score: The linguistic scoreindicates thelinguistic likelihood of word strings in the sentence and ismeasured by n-gram probability(2)In our experiment, trigram probability calculated usingtranscriptions of presentation utterances in the CSJ consisting of 1.5 M morphemes (words) is used. This scorede-weights linguistically unnatural word strings caused byrecognition errors.indicatesSignificance score: The significance scorethe significance of each wordin the sentence and ismeasured by the amount of information. The amount of information contained in each word is calculated for contentwords including nouns, verbs, adjectives and out-of-vocabulary (OOV) words, based on word occurrence in a corpusas shown in (3). The POS information for each word is obtained from the recognition result, since every word in thedictionary is accompanied with a unique POS tag. A flatscore is given to other words, and(3)where is the number of occurrences ofin the recognized utterances, is the number of occurrences ofinis the number of all contenta large-scale corpus, and.words in that corpus, that isFor measuring the significance score, the number ofoccurrences of 120 000 kinds of words is calculated ina corpus consisting of transcribed presentations (1.5 Mwords), proceedings of 60 presentations, presentationrecords obtained from the World-Wide Web (WWW)(2.1 M words), NHK (Japanese broadcast company)broadcast news text (22 M words), Mainichi newspapertext (87 M words) and text from a speech textbook“Speech Information Processing” (51 000 words). Important keywords are weighted and the words unrelatedto the original content, such as recognition errors, arede-weighted by this score.is incorConfidence score: The confidence scoreporated to weight acoustically as well as linguistically reliable hypotheses. Specifically, a logarithmic value of theposterior probability for each transcribed word, which isthe ratio of a word hypothesis probability to that of all otherhypotheses, is calculated using a word graph obtained by adecoder and used as a confidence score.2) Sentence Compaction: After removing relatively lessimportant sentences, the remaining transcription is automatically modified into a written editorial article style tocalculate the score for sentence compaction. All the sentencesare concatenated while preserving sentence boundaries, anda linguistic score,, a significance score, and aare given to each transcribed word. Aconfidence scorefor every combination ofword concatenation scorewords within each transcribed sentence is also given to weight403a word concatenation between words. This score is a measureof the dependency between two words and is obtained by aphrase structure grammar, stochastic dependency context-freegrammar (SDCFG). A set of words that maximizes a weightedsum of these scores is selected according to a given compression ratio and connected to create a summary using a two-stagedynamic programming (DP) technique. Specifically, eachsentence is summarized according to all possible compressionratios, and then the best combination of summarized sentencesis determined according to a target total compression ratio.Ideally, the linguistic score should be calculated using a wordconcatenation model based on a large-scale summary corpus.Since such a summary corpus is not yet available, the transcribed presentations used to calculate the word trigrams for theimportant sentence extraction are automatically modified intoa written editorial article style and used together with the proceedings of 60 presentations to calculate the trigrams.The significance score is calculated using the same corpusas that used for calculating the score for important sentenceextraction. The word-dependency probability is estimated bythe Inside-Outside algorithm, using a manually parsed Mainichinewspaper corpus having 4 M sentences with 68 M words. Forthe details of the SDCFG and dependency scores, readers shouldrefer to [12].B. Evaluation Experiments1) Evaluation Set: Three presentations, M74, M35, andM31, in the CSJ by male speakers were summarized atsummarization ratios of 70% and 50%. The summarizationratio was defined as the ratio of the number of charactersin the summaries to that in the recognition results. Table Ishows features of the presentations, that is, length, mean wordrecognition accuracy, number of sentences, number of words,number of fillers, filler ratio, and number of disfluenciesincluding repairs of each presentation. They were manuallysegmented into sentences before recognition. The table showsthat the presentation M35 has a significantly large number ofdisfluencies and a low recognition accuracy, and M31 has asignificantly high filler ratio.2) Summarization Accuracy: To objectively evaluate thesummaries, correctly transcribed presentation speech wasmanually summarized by nine human subjects to create targets.Devising meaningful evaluation criteria and metrics for speechsummarization is a problematic issue. Speech does not haveexplicit sentence boundaries in contrast with text input. Therefore, speech summarization results cannot be evaluated usingthe F-measure based on sentence units. In addition, since words(morphemes) within sentences are extracted and concatenatedin the summarization process, variations of target summariesmade by human subjects are much larger than those using thesentence level method. In almost all cases, an “ideal” summarydoes not exist. For these reasons, variations of the manualsummarization results were merged into a word network asshown in Fig. 2, which is considered to approximately expressall possible correct summaries covering subjective variations.Word accuracy of the summary is then measured in comparisonwith the closest word string extracted from the word networkas the summarization accuracy [5].404IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 12, NO. 4, JULY 2004TABLE IEVALUATION SETFig. 2.Word network made by merging manual summarization results.3) Evaluation Conditions: Summarization was performedunder the following nine conditions: single-stage summarization without applying the important sentence extraction (NOS);two-stage summarization using seven kinds of the possiblecombination of scores for important sentence extraction ( ,, ,,,,); and summarization by randomandwere set atword selection. The weighting factorsoptimum values for each experimental condition.C. Evaluation Results1) Summarization Accuracy: Results of the evaluation experiments are shown in Figs. 3 and 4. In all the automaticsummarization conditions, both the one-stage method withoutsentence extraction and the two-stage method including sentence extraction achieve better results than random word selection. In both the 70% and 50% summarization conditions,the two-stage method achieves higher summarization accuracythan the one-stage method. The two-stage method is more effective in the condition of the smaller summarization ratio(50%), that is, where there is a higher compression ratio, thanin the condition of the larger summarization ratio (70%). Inthe 50% summarization condition, the two-stage method iseffective for all three presentations. The two-stage method isespecially effective for avoiding one of the problems of theone-stage method, that is, the production of short unreadableand/or incomprehensible sentences.Comparing the three scores for sentence extraction, the sigis more effective than the linguistic scorenificance scoreand the confidence score. The summarization score can be,,increased by using the combination of two scores (), and even more by combining all three scores.Fig. 3. Results of the summarization with text presentation at 50%summarization ratio.Fig. 4. Results of the summarization with text presentation at 70%summarization ratio.FURUI et al.: SPEECH-TO-TEXT AND SPEECH-TO-SPEECH SUMMARIZATIONThe differences are, however, statistically insignificant in theseexperiments, due to the limited size of the data.2) Effects of the Ratio of Compression by Sentence Extraction: Figs. 5 and 6 show the summarization accuracy as afunction of the ratio of compression by sentence extraction forthe total summarization ratios of 50% or 70%. The left andright ends of the figures correspond to summarizations by onlysentence compaction and sentence extraction, respectively.These results indicate that although the best summarizationaccuracy of each presentation can be obtained at a differentratio of compression by sentence extraction, there is a generaltendency where the smaller the summarization ratio becomes,the larger the optimum ratio of compression by sentenceextraction becomes. That is, sentence extraction becomes moreeffective when the summarization ratio gets smaller.Comparing results at the left and right ends of the figures,summarization by word extraction (i.e., sentence compaction)is more effective than sentence extraction for the M35 presentation. This presentation includes a relatively large amount of redundant information, such as disfluencies and repairs, and hasa significantly low recognition accuracy. These results indicatethat the optimum division of the compression ratio into the twosummarization stages needs to be estimated according to thespecific summarization ratio and features of the presentation inquestion, such as frequency of disfluencies.405Fig. 5. Summarization accuracy as a function of the ratio of compression bysentence extraction for the total summarization ratio of 50%.III. SUMMARIZATION WITH SPEECH PRESENTATIONA. Unit Selection and Concatenation1) Units for Extraction: The following issues need to be addressed in extracting and concatenating speech segments formaking summaries.1) Units for extraction: sentences, phrases, or words.2) Criteria for measuring the importance of units forextraction.3) Concatenation methods for making summary speech.The following three units are investigated in this paper: sentences, words, and between-filler units. All the fillers automatically detected as the result of recognition are removed beforeextracting important segments.Sentence units: The method described in Section II-A.1is applied to the recognition results to extract importantsentences. Since sentences are basic linguistic as well asacoustic units, it is easy to maintain acoustical smoothnessby using sentences as units, and therefore the concatenatedspeech sounds natural. However, since the units are relatively long, they tend to include unnecessary words. Sincefillers are automatically removed even if they are includedwithin sentences as described above, the sentences are cutand shortened at the position of fillers.Word units: Word sets are extracted and concatenated byapplying the method described in Section II-A.2 to therecognition results. Although this method has an advantage in that important parts can be precisely extracted insmall units, it tends to cause acoustical discontinuity sincemany small units of speech need to be concatenated. Therefore, summarization speech made by this method sometimes sounds unnatural.Fig. 6. Summarization accuracy as a function of the ratio of compression bysentence extraction for the total summarization ratio of 70%.Between-filler units: Speech segments between fillers aswell as sentence boundaries are extracted using speechrecognition results. The same method as that used for extracting sentence units is applied to evaluate these units.These units are introduced as intermediate units betweensentences and words, in anticipation of both reasonablyprecise extraction of important parts and naturalness ofspeech with acoustic continuity.2) Unit Concatenation: Units for building summarizationspeech are extracted from original speech by using segmentationboundaries obtained from speech recognition results. When theunits are concatenated at the inside of sentences, it may producenoise due to a difference of amplitudes of the speech waveforms.In order to avoid this problem, amplitudes of approximately20-ms length at the unit boundaries are gradually attenuatedbefore the concatenation. Since this causes an impression of406IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 12, NO. 4, JULY 2004TABLE IISUMMARIZATION ACCURACY AND NUMBER OF UNITS FOR THE THREE KINDS OF SUMMARIZATION UNITSincreasing the speaking rate and thus creates an unnaturalsound, a short pause is inserted. The length of the pauseis controlled between 50 and 100 ms empirically accordingto the concatenation conditions. Each summarization speechwhich has been made by this method is hereafter referred to as“summarization speech sentence” and the text correspondingto its speech period is referred to as “summarization textsentence.”The summarization speech sentences are further concatenated to create a summarized speech for the whole presentation.Speech waveforms at sentence boundaries are gradually attenuated and pauses are inserted between the sentences in thesame way as the unit concatenation within sentences. Short andlong pauses with 200- and 700-ms lengths are used as pausesbetween sentences. Long pauses are inserted after sentenceending expressions, otherwise short pauses are used. In the caseof summarization by word-unit concatenation, long pauses arealways used, since many sentences terminate with nouns andneed relatively long pauses to make them sound natural.Fig. 7. Evaluation results for the summarization with speech presentation interms of the ease of understanding.B. Evaluation Experiments1) Experimental Conditions: The three presentations,M74, M35, and M31, were automatically summarized with asummarization ratio of 50%. Summarization accuracies for thethree presentations using sentence units, between-filler units,and word units, are given in Table II. Manual summaries madeby nine human subjects were used for the evaluation. The tablealso shows the number of automatically detected units in eachcondition. For the case of using the between-filler units, thenumber of detected fillers is also shown.Using the summarization text sentences, speech segmentswere extracted and concatenated to build summarizationspeech, and subjective evaluation by 11 subjects was performedin terms of ease of understanding and appropriateness as a summarization with five levels: 1—very bad; 2—bad; 3—normal;4—good; and 5—very good. The subjects were instructed toread the transcriptions of the presentations and understand thecontents before hearing the summarization speech.Fig. 8. Evaluation results for the summarization with speech presentation interms of the appropriateness as a summary.2) Evaluation Results and Discussion: Figs. 7 and 8 showthe evaluation results. Averaging over the three presentations,the sentence units show the best results whereas the word unitsFURUI et al.: SPEECH-TO-TEXT AND SPEECH-TO-SPEECH SUMMARIZATIONshow the worst. For the two presentations, M74 and M35, thebetween-filler units achieve almost the same results as the sentence units. The reason why the word units which show slightlybetter summarization accuracy in Table II also show the worstsubjective evaluation results here is because of unnatural sounddue to the concatenation of short speech units. The relativelylarge number of fillers included in the presentation M31 produced many short units when the between-filler unit method wasapplied. This is the reason why between-filler units show worsesubjective results than the sentence units for M31.If the summarization ratio is set lower than 50%, betweenfiller units are expected to achieve better results than sentenceunits, since sentence units cannot remove redundant expressionswithin sentences.IV. CONCLUSIONIn this paper, we have presented techniques for compaction-based automatic speech summarization and evaluationresults for summarizing spontaneous presentations. The summarization results are presented by either text or speech. In theformer case, the speech-to-test summarization, we proposed atwo-stage automatic speech summarization method consistingof important sentence extraction and word-based sentencecompaction. In this method, inadequate sentences includingrecognition errors and less important information are automatically removed before sentence compaction. It was confirmedthat in spontaneous presentation speech summarization at 70%and 50% summarization ratios, combining sentence extractionwith sentence compaction is effective; this method achievesbetter summarization performance than our previous one-stagemethod. It was also confirmed that three scores, the linguisticscore, the word significance score and the word confidencescore, are effective for extracting important sentences. Thebest division for the summarization ratio into the ratios ofsentence extraction and sentence compaction depends on thesummarization ratio and features of presentation utterances.For the case of presenting summaries by speech, thespeech-to-speech summarization, three kinds of units—sentences, words, and between-filler units—were investigated asunits to be extracted from original speech and concatenatedto produce the summaries. A set of units is automaticallyextracted using the same measures used in the speech-to-textsummarization, and the speech segments corresponding to theextracted units are concatenated to produce the summaries.Amplitudes of speech waveforms at the boundaries are gradually attenuated and pauses are inserted before concatenationto avoid acoustic discontinuity. Subjective evaluation resultsfor the 50% summarization ratio indicated that sentence unitsachieve the best subjective evaluation score. Between-fillerunits are expected to achieve good performance when thesummarization ratio becomes smaller.As stated in the introduction, speech summarization technology can be applied to any kind of speech document and isexpected to play an important role in building various speecharchives including broadcast news, lectures, presentations, andinterviews. Summarization and question answering (QA) perform a similar task, in that they both map an abundance ofinformation to a (much) smaller piece to be presented to the407user [17]. Therefore, speech summarization research will helpthe advancement of QA systems using speech documents. Bycondensing important points of long presentations and lectures,speech-to-speech summarization can provide the listener witha valuable means for absorbing much information in a muchshorter time.Future research includes evaluation by a large number ofpresentations at various summarization ratios including smallerratios, investigation of other information/features for important unit extraction, methods for automatically segmenting apresentation into sentence units [16], those methods’ effectson summarization accuracy, and automatic optimization ofthe division of compression ratio into the two summarizationstages according to the summarization ratio and features ofthe presentation.ACKNOWLEDGMENTThe authors would like to thank NHK (Japan BroadcastingCorporation) for providing the broadcast news database.REFERENCES[1] S. Furui, K. Iwano, C. Hori, T. Shinozaki, Y. Saito, and S. Tamura,“Ubiquitous speech processing,” in Proc. ICASSP2001, vol. 1, Salt LakeCity, UT, 2001, pp. 13–16.[2] S. Furui, “Recent advances in spontaneous speech recognition and understanding,” in Proc. ISCA-IEEE Workshop on Spontaneous SpeechProcessing and Recognition, Tokyo, Japan, 2003.[3] I. Mani and M. T. Maybury, Eds., Advances in Automatic Text Summarization. Cambridge, MA: MIT Press, 1999.[4] J. Alexandersson and P. Poller, “Toward multilingual protocol generation for spontaneous dialogues,” in Proc. INLG-98, Niagara-on-the-lake,Canada, 1998.[5] K. Zechner and A. Waibel, “Minimizing word error rate in textual summaries of spoken language,” in Proc. NAACL, Seattle, WA, 2000.[6] J. S. Garofolo, E. M. Voorhees, C. G. P. Auzanne, and V. M. Stanford,“Spoken document retrieval: 1998 evaluation and investigation of newmetrics,” in Proc. ESCA Workshop: Accessing Information in SpokenAudio, Cambridge, MA, 1999, pp. 1–7.[7] R. Valenza, T. Robinson, M. Hickey, and R. Tucker, “Summarization ofspoken audio through information extraction,” in Proc. ISCA Workshopon Accessing Information in Spoken Audio, Cambridge, MA, 1999, pp.111–116.[8] K. Koumpis and S. Renals, “Transcription and summarization of voicemail speech,” in Proc. ICSLP 2000, 2000, pp. 688–691.[9] K. Maekawa, H. Koiso, S. Furui, and H. Isahara, “Spontaneous speechcorpus of Japanese,” in Proc. LREC2000, Athens, Greece, 2000, pp.947–952.[10] T. Kikuchi, S. Furui, and C. Hori, “Two-stage automatic speech summarization by sentence extraction and compaction,” in Proc. ISCA-IEEEWorkshop on Spontaneous Speech Processing and Recognition, Tokyo,Japan, 2003.[11] C. Hori and S. Furui, “Advances in automatic speech summarization,”in Proc. Eurospeech 2001, 2001, pp. 1771–1774.[12] C. Hori, S. Furui, R. Malkin, H. Yu, and A. Waibel, “A statistical approach to automatic speech summarization,” EURASIP J. Appl. SignalProcessing, pp. 128–139, 2003.[13] K. Knight and D. Marcu, “Summarization beyond sentence extraction:A probabilistic approach to sentence compression,” Artific. Intell., vol.139, pp. 91–107, 2002.[14] H. Daume III and D. Marcu, “A noisy-channel model for document compression,” in Proc. ACL-2002, Philadelphia, PA, 2002, pp. 449–456.[15] C.-Y. Lin and E. Hovy, “From single to multi-document summarization:A prototype system and its evaluation,” in Proc. ACL-2002, Philadelphia, PA, 2002, pp. 457–464.[16] M. Hirohata, Y. Shinnaka, and S. Furui, “A study on important sentenceextraction methods using SVD for automatic speech summarization,”in Proc. Acoustical Society of Japan Autumn Meeting, Nagoya, Japan,2003.[17] K. Zechner, “Spoken language condensation in the 21st Century,” inProc. Eurospeech, Geneva, Switzerland, 2003, pp. 1989–1992.408IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 12, NO. 4, JULY 2004Sadaoki Furui (F’93) is a Professor at the Department of Computer Science, Tokyo Instituteof Technology, Tokyo, Japan. He is engaged in awide range of research on speech analysis, speechrecognition, speaker recognition, speech synthesis,and multimodal human-computer interaction andhas authored or coauthored over 350 publishedarticles. From 1978 to 1979, he served on staff atthe Acoustics Research Department of Bell Laboratories, Murray Hill, NJ, as a Visiting Researcher,working on speaker verification. He is the author ofDigital Speech Processing, Synthesis, and Recognition (New York: MarcelDekker, 1989; revised, 2000), Digital Speech Processing (Tokai, Japan:Tokai Univ. Press, 1985), Acoustics and Speech Processing (Tokyo, Japan:Kindai-Kagaku-Sha, 1992) in Japanese, and Speech Information Processing(Tokyo, Japan: Morikita, 1998). He edited (with M. M. Sondhi) Advances inSpeech Signal Processing (New York: Marcel Dekker, 1992). He has translatedinto Japanese Fundamentals of Speech Recognition (Tokyo, Japan: NTTAdvanced Technology, 1995), authored by L. R. Rabiner and B.-H. Juang,and Vector Quantization and Signal Compression (Tokyo, Japan: Corona-sha,1998), authored by A. Gersho and R. M. Gray.Dr. Furui is a Fellow of the Acoustical Society of America and the Institute ofElectronics, Information and Communication Engineers of Japan (IEICE). Heis President of the Acoustical Society of Japan (ASJ), the International SpeechCommunication Association (ISCA), and the Permanent Council for International Conferences on Spoken Language Processing (PC-ICSLP). He is a Boardof Governor of the IEEE Signal Processing Society (SPS) and in 1993, he servedas an IEEE SPS Distinguished Lecturer. He has served on the IEEE TechnicalCommittee on Speech and MMSP and on numerous IEEE Conference organizing committees. He is Editor-in-Chief of the Transactions of the IEICE. Heis also an Editorial Board member of Speech Communication, the Journal ofComputer Speech and Language, and the Journal of Digital Signal Processing.He has received numerous awards, including: the Yonezawa Prize and the PaperAwards from the IEICE (1975, 1988, 1993, and 2003); the Sato Paper Awardfrom the ASJ (1985 and 1987); the Senior Award from the IEEE Acoustics,Speech, and Signal Processing Society (1989); the Achievement Award fromthe Minister of Science and Technology of Japan (1989); the Technical Achievement Award and the Book Award from the IEICE (1990 and 2003); the Mira PaulMemorial Award from the AFECT of India (2001).Tomonori Kikuchi received the B. E. and M. E.degrees in computer science from Tokyo Instituteof Technology, Tokyo, Japan, in 2001 and 2003,respectively.He has been with Japan Patent Office, Tokyo,Japan, since 2003.Yousuke Shinnaka received the B. E. degree inelectrical and electronic engineering from TokyoInstitute of Technology, Tokyo, Japan, in 2003. Heis currently pursuing the M.S. degree at TokyoInstitute of Technology.Chiori Hori (M’02) received the B.E. and M.E.degrees in electrical and information engineeringfrom Yamagata University, Yonezawa, Japan, in1994 and 1997, respectively, and the Ph.D. degreefrom the Graduate School of Information Scienceand Engineering, Tokyo Institute of Technology(TITECH), Tokyo, Japan, in 2002.From April 1997 to March 1999, she was aResearch Associate with the Faculty of Literatureand Social Sciences, Yamagata University. She iscurrently a Researcher with NTT CommunicationScience Laboratories (CS Labs), Nippon Telegraph and Telephone Corporation(NTT), Kyoto, Japan, which she joined in 2002.Dr. Hori is a member of the Acoustical Society of Japan (ASJ), the Instituteof Electronics, Information and Communication Engineers of Japan (IEICE),and the Information Processing Society of Japan (IPSJ). She received the PaperAward from the IEICE in 2002 for her work on speech summarization.
Conclusion : In this paper, we have presented techniques for compaction-based automatic speech summarization and evaluationresults for summarizing spontaneous presentations. The summarization results are presented by either text or speech. In theformer case, the speech-to-test summarization, we proposed atwo-stage automatic speech summarization method consistingof important sentence extraction and word-based sentencecompaction. In this method, inadequate sentences includingrecognition errors and less important information are automatically removed before sentence compaction. It was confirmedthat in spontaneous presentation speech summarization at 70%and 50% summarization ratios, combining sentence extractionwith sentence compaction is effective; this method achievesbetter summarization performance than our previous one-stagemethod. It was also confirmed that three scores, the linguisticscore, the word significance score and the word confidencescore, are effective for extracting important sentences. Thebest division for the summarization ratio into the ratios ofsentence extraction and sentence compaction depends on thesummarization ratio and features of presentation utterances.For the case of presenting summaries by speech, thespeech-to-speech summarization, three kinds of units—sentences, words, and between-filler units—were investigated asunits to be extracted from original speech and concatenatedto produce the summaries. A set of units is automaticallyextracted using the same measures used in the speech-to-textsummarization, and the speech segments corresponding to theextracted units are concatenated to produce the summaries.Amplitudes of speech waveforms at the boundaries are gradually attenuated and pauses are inserted before concatenationto avoid acoustic discontinuity. Subjective evaluation resultsfor the 50% summarization ratio indicated that sentence unitsachieve the best subjective evaluation score. Between-fillerunits are expected to achieve good performance when thesummarization ratio becomes smaller.As stated in the introduction, speech summarization technology can be applied to any kind of speech document and isexpected to play an important role in building various speecharchives including broadcast news, lectures, presentations, andinterviews. Summarization and question answering (QA) perform a similar task, in that they both map an abundance ofinformation to a (much) smaller piece to be presented to the407user [17]. Therefore, speech summarization research will helpthe advancement of QA systems using speech documents. Bycondensing important points of long presentations and lectures,speech-to-speech summarization can provide the listener witha valuable means for absorbing much information in a muchshorter time.Future research includes evaluation by a large number ofpresentations at various summarization ratios including smallerratios, investigation of other information/features for important unit extraction, methods for automatically segmenting apresentation into sentence units [16], those methods’ effectson summarization accuracy, and automatic optimization ofthe division of compression ratio into the two summarizationstages according to the summarization ratio and features ofthe presentation.
Discussion : The authors would like to thank NHK (Japan BroadcastingCorporation) for providing the broadcast news database.
Biblio : [1] S. Furui, K. Iwano, C. Hori, T. Shinozaki, Y. Saito, and S. Tamura, “Ubiquitous speech processing,” in Proc. ICASSP2001, vol. 1, Salt Lake City, UT, 2001, pp. 13–16  
  [2] S. Furui, “Recent advances in spontaneous speech recognition and understanding,” in Proc. ISCA-IEEE Workshop on Spontaneous Speech Processing and Recognition, Tokyo, Japan, 2003  
  [3] I. Mani and M. T. Maybury, Eds., Advances in Automatic Text Summarization. Cambridge, MA: MIT Press, 1999  
  [4] J. Alexandersson and P. Poller, “Toward multilingual protocol generation for spontaneous dialogues,” in Proc. INLG-98, Niagara-on-the-lake, Canada, 1998  
  [5] K. Zechner and A. Waibel, “Minimizing word error rate in textual summaries of spoken language,” in Proc. NAACL, Seattle, WA, 2000  
  [6] J. S. Garofolo, E. M. Voorhees, C. G. P. Auzanne, and V. M. Stanford, “Spoken document retrieval: 1998 evaluation and investigation of new metrics,” in Proc. ESCA Workshop: Accessing Information in Spoken Audio, Cambridge, MA, 1999, pp. 1–7  
  [7] R. Valenza, T. Robinson, M. Hickey, and R. Tucker, “Summarization of spoken audio through information extraction,” in Proc. ISCA Workshop on Accessing Information in Spoken Audio, Cambridge, MA, 1999, pp  
  111–116  
  [8] K. Koumpis and S. Renals, “Transcription and summarization of voicemail speech,” in Proc. ICSLP 2000, 2000, pp. 688–691  
  [9] K. Maekawa, H. Koiso, S. Furui, and H. Isahara, “Spontaneous speech corpus of Japanese,” in Proc. LREC2000, Athens, Greece, 2000, pp  
  947–952  
  [10] T. Kikuchi, S. Furui, and C. Hori, “Two-stage automatic speech summarization by sentence extraction and compaction,” in Proc. ISCA-IEEE Workshop on Spontaneous Speech Processing and Recognition, Tokyo, Japan, 2003  
  [11] C. Hori and S. Furui, “Advances in automatic speech summarization,” in Proc. Eurospeech 2001, 2001, pp. 1771–1774  
  [12] C. Hori, S. Furui, R. Malkin, H. Yu, and A. Waibel, “A statistical approach to automatic speech summarization,” EURASIP J. Appl. Signal Processing, pp. 128–139, 2003  
  [13] K. Knight and D. Marcu, “Summarization beyond sentence extraction: A probabilistic approach to sentence compression,” Artific. Intell., vol  
  139, pp. 91–107, 2002  
  [14] H. Daume III and D. Marcu, “A noisy-channel model for document compression,” in Proc. ACL-2002, Philadelphia, PA, 2002, pp. 449–456  
  [15] C.-Y. Lin and E. Hovy, “From single to multi-document summarization: A prototype system and its evaluation,” in Proc. ACL-2002, Philadelphia, PA, 2002, pp. 457–464  
  [16] M. Hirohata, Y. Shinnaka, and S. Furui, “A study on important sentence extraction methods using SVD for automatic speech summarization,” in Proc. Acoustical Society of Japan Autumn Meeting, Nagoya, Japan, 2003  
  [17] K. Zechner, “Spoken language condensation in the 21st Century,” in Proc. Eurospeech, Geneva, Switzerland, 2003, pp. 1989–1992  
   408  IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 12, NO. 4, JULY 2004  Sadaoki Furui (F’93) is a Professor at the Department of Computer Science, Tokyo Institute of Technology, Tokyo, Japan. He is engaged in a wide range of research on speech analysis, speech recognition, speaker recognition, speech synthesis, and multimodal human-computer interaction and has authored or coauthored over 350 published articles. From 1978 to 1979, he served on staff at the Acoustics Research Department of Bell Laboratories, Murray Hill, NJ, as a Visiting Researcher, working on speaker verification. He is the author of Digital Speech Processing, Synthesis, and Recognition (New York: Marcel Dekker, 1989
 revised, 2000), Digital Speech Processing (Tokai, Japan: Tokai Univ. Press, 1985), Acoustics and Speech Processing (Tokyo, Japan: Kindai-Kagaku-Sha, 1992) in Japanese, and Speech Information Processing (Tokyo, Japan: Morikita, 1998). He edited (with M. M. Sondhi) Advances in Speech Signal Processing (New York: Marcel Dekker, 1992). He has translated into Japanese Fundamentals of Speech Recognition (Tokyo, Japan: NTT Advanced Technology, 1995), authored by L. R. Rabiner and B.-H. Juang, and Vector Quantization and Signal Compression (Tokyo, Japan: Corona-sha, 1998), authored by A. Gersho and R. M. Gray  
  Dr. Furui is a Fellow of the Acoustical Society of America and the Institute of Electronics, Information and Communication Engineers of Japan (IEICE). He is President of the Acoustical Society of Japan (ASJ), the International Speech Communication Association (ISCA), and the Permanent Council for International Conferences on Spoken Language Processing (PC-ICSLP). He is a Board of Governor of the IEEE Signal Processing Society (SPS) and in 1993, he served as an IEEE SPS Distinguished Lecturer. He has served on the IEEE Technical Committee on Speech and MMSP and on numerous IEEE Conference organizing committees. He is Editor-in-Chief of the Transactions of the IEICE. He is also an Editorial Board member of Speech Communication, the Journal of Computer Speech and Language, and the Journal of Digital Signal Processing  
  He has received numerous awards, including: the Yonezawa Prize and the Paper Awards from the IEICE (1975, 1988, 1993, and 2003)
 the Sato Paper Award from the ASJ (1985 and 1987)
 the Senior Award from the IEEE Acoustics, Speech, and Signal Processing Society (1989)
 the Achievement Award from the Minister of Science and Technology of Japan (1989)
 the Technical Achievement Award and the Book Award from the IEICE (1990 and 2003)
 the Mira Paul Memorial Award from the AFECT of India (2001)  
   Tomonori Kikuchi received the B. E. and M. E  
  degrees in computer science from Tokyo Institute of Technology, Tokyo, Japan, in 2001 and 2003, respectively  
  He has been with Japan Patent Office, Tokyo, Japan, since 2003  
   Yousuke Shinnaka received the B. E. degree in electrical and electronic engineering from Tokyo Institute of Technology, Tokyo, Japan, in 2003. He is currently pursuing the M.S. degree at Tokyo Institute of Technology  
   Chiori Hori (M’02) received the B.E. and M.E  
  degrees in electrical and information engineering from Yamagata University, Yonezawa, Japan, in 1994 and 1997, respectively, and the Ph.D. degree from the Graduate School of Information Science and Engineering, Tokyo Institute of Technology (TITECH), Tokyo, Japan, in 2002  
  From April 1997 to March 1999, she was a Research Associate with the Faculty of Literature and Social Sciences, Yamagata University. She is currently a Researcher with NTT Communication Science Laboratories (CS Labs), Nippon Telegraph and Telephone Corporation (NTT), Kyoto, Japan, which she joined in 2002  
  Dr. Hori is a member of the Acoustical Society of Japan (ASJ), the Institute of Electronics, Information and Communication Engineers of Japan (IEICE), and the Information Processing Society of Japan (IPSJ). She received the Paper Award from the IEICE in 2002 for her work on speech summarization  
   