Nom du fichier d’origine : Metsis 2006 Spam filtering with naive bayes-which naive bayes



Titre du papier  : Spam Filtering with Naive Bayes – Which Naive Bayes?

Auteur : Spam Filtering with Naive Bayes – Which Naive Bayes?


Abstract de l’auteur : Naive Bayes is very popular in commercial and open-sourceanti-spam e-mail filters. There are, however, several formsof Naive Bayes, something the anti-spam literature does notalways acknowledge. We discuss five different versions ofNaive Bayes, and compare them on six new, non-encodeddatasets, that contain ham messages of particular Enronusers and fresh spam messages. The new datasets, whichwe make publicly available, are more realistic than previouscomparable benchmarks, because they maintain the temporal order of the messages in the two categories, and theyemulate the varying proportion of spam and ham messagesthat users receive over time. We adopt an experimentalprocedure that emulates the incremental training of personalized spam filters, and we plot roc curves that allow us tocompare the different versions of nb over the entire tradeoffbetween true positives and true negatives.
Introduction : 
Although several machine learning algorithms have been
employed in anti-spam e-mail filtering, including algorithms
that are considered top-performers in text classification, like
Boosting and Support Vector Machines (see, for example,
[4, 6, 10, 16]), Naive Bayes (nb) classifiers currently appear
to be particularly popular in commercial and open-source
spam filters. This is probably due to their simplicity, which
makes them easy to implement, their linear computational
complexity, and their accuracy, which in spam filtering is
comparable to that of more elaborate learning algorithms
[2]. There are, however, several forms of nb classifiers, and
the anti-spam literature does not always acknowledge this.
In their seminal papers on learning-based spam filters,
Sahami et al. [21] used a nb classifier with a multi-variate
Bernoulli model (this is also the model we had used in [1]), a
form of nb that relies on Boolean attributes, whereas Pantel
and Lin [19] in effect adopted the multinomial form of nb,
which normally takes into account term frequencies. McCallum and Nigam [17] have shown experimentally that the
∗This version of the paper contains some minor corrections
in the description of Flexible Bayes, which were made after
the conference.
†Work carried out mostly while at the Department of Informatics, Athens University of Economics and Business.

CEAS 2006 - Third Conference on Email and Anti-Spam, July 27-28, 2006,
Mountain View, California USA

∗

multinomial nb performs generally better than the multivariate Bernoulli nb in text classification, a finding that
Schneider [24] and Hovold [12] verified with spam filtering experiments on Ling-Spam and the pu corpora [1, 2,
23]. In further work on text classification, which included
experiments on Ling-Spam, Schneider [25] found that the
multinomial nb surprisingly performs even better when term
frequencies are replaced by Boolean attributes.
The multi-variate Bernoulli nb can be modified to accommodate continuous attributes, leading to what we call the
multi-variate Gauss nb, by assuming that the values of each
attribute follow a normal distribution within each category
[14]. Alternatively, the distribution of each attribute in each
category can be taken to be the average of several normal
distributions, one for every different value the attribute has
in the training data of that category, leading to a nb version that John and Langley [14] call Flexible Bayes (fb).
In previous work [2], we found that fb clearly outperforms
the multi-variate Gauss nb on the pu corpora, when the attributes are term frequencies divided by document lengths,
but we did not compare fb against the other nb versions.
In this paper we shed more light on the five versions of
nb mentioned above, and we evaluate them experimentally
on six new, non-encoded datasets, collectively called EnronSpam, which we make publicly available.1 Each dataset contains ham (non-spam) messages from a single user of the
Enron corpus [15], to which we have added fresh spam messages with varying ham-spam ratios. Although a similar
approach was adopted in the public benchmark of the trec
2005 Spam Track, to be discussed below, we believe that
our datasets are better suited to evaluations of personalized
filters, i.e., filters that are trained on incoming messages of
a particular user they are intended to protect, which is the
type of filters the experiments of this paper consider. Unlike Ling-Spam and the pu corpora, in the new datasets we
maintain the order in which the original messages of the
two categories were received, and we emulate the varying
proportion of ham and spam messages that users receive
over time. This allows us to conduct more realistic experiments, and to take into account the incremental training
of personal filters. Furthermore, rather than focussing on a
handful of relative misclassification costs (cost of false positives vs. false negatives; λ = 1, 9, 999 in our previous work),
1
The
Enron-Spam
datasets
are
available
from
http://www.iit.demokritos.gr/skel/i-config/
and
http://www.aueb.gr/users/ion/publications.html
in
both raw and pre-processed form. Ling-Spam and the pu
corpora are also available from the same addresses.

we plot entire roc curves, which allow us to compare the
different versions of nb over the entire tradeoff between true
positives and true negatives.
Note that several publicly available spam filters appear
to be using techniques described as “Bayesian”, but which
are very different from any form of nb discussed in the academic literature and any other technique that would normally
be called Bayesian therein.2 Here we focus on nb versions
published in the academic literature, leaving comparisons
against other “Bayesian” techniques for future work.
Section 2 below presents the event models and assumptions of the nb versions we considered. Section 3 explains
how the datasets of our experiments were assembled and
the evaluation methodology we used; it also highlights some
pitfalls that have to be avoided when constructing spam filtering benchmarks. Section 4 then presents and discusses
our experimental results. Section 5 concludes and provides
directions for further work.


Corps : Although several machine learning algorithms have beenemployed in anti-spam e-mail filtering, including algorithmsthat are considered top-performers in text classification, likeBoosting and Support Vector Machines (see, for example,[4, 6, 10, 16]), Naive Bayes (nb) classifiers currently appearto be particularly popular in commercial and open-sourcespam filters. This is probably due to their simplicity, whichmakes them easy to implement, their linear computationalcomplexity, and their accuracy, which in spam filtering iscomparable to that of more elaborate learning algorithms[2]. There are, however, several forms of nb classifiers, andthe anti-spam literature does not always acknowledge this.In their seminal papers on learning-based spam filters,Sahami et al. [21] used a nb classifier with a multi-variateBernoulli model (this is also the model we had used in [1]), aform of nb that relies on Boolean attributes, whereas Panteland Lin [19] in effect adopted the multinomial form of nb,which normally takes into account term frequencies. McCallum and Nigam [17] have shown experimentally that the∗This version of the paper contains some minor correctionsin the description of Flexible Bayes, which were made afterthe conference.†Work carried out mostly while at the Department of Informatics, Athens University of Economics and Business.CEAS 2006 - Third Conference on Email and Anti-Spam, July 27-28, 2006,Mountain View, California USA∗multinomial nb performs generally better than the multivariate Bernoulli nb in text classification, a finding thatSchneider [24] and Hovold [12] verified with spam filtering experiments on Ling-Spam and the pu corpora [1, 2,23]. In further work on text classification, which includedexperiments on Ling-Spam, Schneider [25] found that themultinomial nb surprisingly performs even better when termfrequencies are replaced by Boolean attributes.The multi-variate Bernoulli nb can be modified to accommodate continuous attributes, leading to what we call themulti-variate Gauss nb, by assuming that the values of eachattribute follow a normal distribution within each category[14]. Alternatively, the distribution of each attribute in eachcategory can be taken to be the average of several normaldistributions, one for every different value the attribute hasin the training data of that category, leading to a nb version that John and Langley [14] call Flexible Bayes (fb).In previous work [2], we found that fb clearly outperformsthe multi-variate Gauss nb on the pu corpora, when the attributes are term frequencies divided by document lengths,but we did not compare fb against the other nb versions.In this paper we shed more light on the five versions ofnb mentioned above, and we evaluate them experimentallyon six new, non-encoded datasets, collectively called EnronSpam, which we make publicly available.1 Each dataset contains ham (non-spam) messages from a single user of theEnron corpus [15], to which we have added fresh spam messages with varying ham-spam ratios. Although a similarapproach was adopted in the public benchmark of the trec2005 Spam Track, to be discussed below, we believe thatour datasets are better suited to evaluations of personalizedfilters, i.e., filters that are trained on incoming messages ofa particular user they are intended to protect, which is thetype of filters the experiments of this paper consider. Unlike Ling-Spam and the pu corpora, in the new datasets wemaintain the order in which the original messages of thetwo categories were received, and we emulate the varyingproportion of ham and spam messages that users receiveover time. This allows us to conduct more realistic experiments, and to take into account the incremental trainingof personal filters. Furthermore, rather than focussing on ahandful of relative misclassification costs (cost of false positives vs. false negatives; λ = 1, 9, 999 in our previous work),1TheEnron-Spamdatasetsareavailablefromhttp://www.iit.demokritos.gr/skel/i-config/andhttp://www.aueb.gr/users/ion/publications.htmlinboth raw and pre-processed form. Ling-Spam and the pucorpora are also available from the same addresses.we plot entire roc curves, which allow us to compare thedifferent versions of nb over the entire tradeoff between truepositives and true negatives.Note that several publicly available spam filters appearto be using techniques described as “Bayesian”, but whichare very different from any form of nb discussed in the academic literature and any other technique that would normallybe called Bayesian therein.2 Here we focus on nb versionspublished in the academic literature, leaving comparisonsagainst other “Bayesian” techniques for future work.Section 2 below presents the event models and assumptions of the nb versions we considered. Section 3 explainshow the datasets of our experiments were assembled andthe evaluation methodology we used; it also highlights somepitfalls that have to be avoided when constructing spam filtering benchmarks. Section 4 then presents and discussesour experimental results. Section 5 concludes and providesdirections for further work.2.NAIVE BAYES CLASSIFIERSAs a simplification, we focus on the textual content ofthe messages. Operational filters would also consider information such as the presence of suspicious headers or tokenobfuscation [11, 21], which can be added as additional attributes in the message representation discussed below. Alternatively, separate classifiers can be trained for textualand other attributes, and then form an ensemble [9, 22].In our experiments, each message is ultimately representedas a vector hx1 , . . . , xm i, where x1 , . . . , xm are the values ofattributes X1 , . . . , Xm , and each attribute provides information about a particular token of the message.3 In thesimplest case, all the attributes are Boolean: Xi = 1 if themessage contains the token; otherwise, Xi = 0. Alternatively, their values may be term frequencies (tf), showinghow many times the corresponding token occurs in the message.4 Attributes with tf values carry more informationthan Boolean ones. Hence, one might expect nb versionsthat use tf attributes to perform better than those withBoolean attributes, an expectation that is not always confirmed, as already mentioned. A third alternative we employed, hereafter called normalized tf, is to divide termfrequencies by the total number of token occurrences in themessage, to take into account the message’s length. Themotivation is that knowing, for example, that “rich” occurs3 times in a message may be a good indication that the message is spam if it is only two paragraphs long, but not if themessage is much longer.Following common text classification practice, we do notassign attributes to tokens that are too rare (we discardtokens that do not occur in at least 5 messages of the training data). We also rank the remaining attributes by information gain, and use only the m best, as in [1, 2, 21],and elsewhere. We experimented with m = 500, 1000, and3000. Note that the information gain ranking treats the at2These techniques derive mostly from P. Graham’s “A planfor spam”; see http://www.paulgraham.com/spam.html.3Attributes may also be mapped to character or token ngrams, but previous attempts to use n-grams in spam filtering led to contradictory or inconclusive results [2, 12, 19].4We treat punctuation and other non-alphabetic charactersas separate tokens. Many of these are highly informative asattributes, because they are more common in spam messages(especially obfuscated ones) than ham messages; see [2].tributes as Boolean, which may not be entirely satisfactorywhen employing a nb version with non-Boolean attributes.Schneider [24] experimented with alternative versions of theinformation gain measure, intended to be more suitable tothe tf-valued attributes of the multinomial nb. His results,however, indicate that the alternative versions do not leadto higher accuracy, although sometimes they allow the samelevel of accuracy to be reached with fewer attributes.From Bayes’ theorem, the probability that a message withvector ~x = hx1 , . . . , xm i belongs in category c is:p(c | ~x) =p(c) · p(~x | c).p(~x)Since the denominator does not depend on the category,nb classifies each message in the category that maximizesp(c) · p(~x | c). In the case of spam filtering, this is equivalentto classifying a message as spam whenever:p(cs ) · p(~x | cs )> T,p(cs ) · p(~x | cs ) + p(ch ) · p(~x | ch )with T = 0.5, where ch and cs denote the ham and spam categories. By varying T , one can opt for more true negatives(correctly classified ham messages) at the expense of fewertrue positives (correctly classified spam messages), or viceversa. The a priori probabilities p(c) are typically estimatedby dividing the number of training messages of category cby the total number of training messages. The probabilitiesp(~x | c) are estimated differently in each nb version.2.1Multi-variate Bernoulli NBLet us denote by F = {t1 , . . . , tm } the set of tokens thatcorrespond to the m attributes after attribute selection. Themulti-variate Bernoulli nb treats each message d as a setof tokens, containing (only once) each ti that occurs ind. Hence, d can be represented by a binary vector ~x =hx1 , . . . , xm i, where each xi shows whether or not ti occurs in d. Furthermore, each message d of category c isseen as the result of m Bernoulli trials, where at each trialwe decide whether or not ti will occur in d. The probability of a positive outcome at trial i (ti occurs in d) isp(ti | c). The multi-variate Bernoulli nb makes the additional assumption that the outcomes of the trials are independent given the category. This is a “naive” assumption,since word co-occurrences in a category are not independent. Similar assumptions are made in all nb versions, andalthough in most cases they are over-simplistic, they stilllead to very good performance in many classification tasks;see, for example, [5] for a theoretical explanation. Then,p(~x | c) can be computed as:Y p(t | c)mp(~x | c) =ixi· (1 − p(ti | c))(1−xi ) ,i=1Qand the criterion for classifying a message as spam becomes:Pmi=1Qp(ti | cs )xi · (1 − p(ti | cs ))(1−xi )> T,mxi · (1 − p(t | c))(1−xi )ic∈{cs ,ch } p(c) ·i=1 p(ti | c)p(cs ) ·where each p(t | c) is estimated using a Laplacean prior as:1 + Mt,c,2 + Mcand Mt,c is the number of training messages of category cthat contain token t, while Mc is the total number of trainingmessages of category c.p(t | c) =2.2Multinomial NB, TF attributesThe multinomial nb with tf attributes treats each message d as a bag of tokens, containing each one of ti as manytimes as it occurs in d. Hence, d can be represented by avector ~x = hx1 , . . . , xm i, where each xi is now the numberof occurrences of ti in d. Furthermore, each message d ofcategory c is seen as the result of picking independently |d|tokens from F with replacement, with probability p(ti | c)for each ti .5 Then, p(~x | c) is the multinomial distribution:Y p(t | c)mp(~x | c) = p(|d|) · |d|! ·ii=1xixi !,where we have followed the common assumption [17, 24,25] that |d| does not depend on the category c. This is anadditional over-simplistic assumption, which is more questionable in spam filtering. For example, the probability ofreceiving a very long spam message appears to be smallerthan that of receiving an equally long ham message.The criterion for classifying a message as spam becomes:PQmi=1Qp(ti | cs )xi> T,mxii=1 p(ti | c)c∈{cs ,ch } p(c) ·p(cs ) ·where each p(t | c) is estimated using a Laplacean prior as:p(t | c) =1 + Nt,c,m + NcPMultinomial NB, Boolean attributesThe multinomial nb with Boolean attributes is the sameas with tf attributes, including the estimates of p(t | c),except that the attributes are now Boolean. It differs fromthe multi-variate Bernoulli nb in that it does not take intoaccount directly the absence (xi = 0) of tokens from themessage (there is no (1 − p(ti | c))(1−xi ) factor), and it estimates the p(t | c) with a different Laplacean prior.It may seem strange that the multinomial nb might perform better with Boolean attributes, which provide less information than tf ones. As Schneider [25] points out, however, it has been proven [7] that the multinomial nb withtf attributes is equivalent to a nb version with attributesmodelled as following Poisson distributions in each category,assuming that the document length is independent of thecategory. Hence, the multinomial nb may perform betterwith Boolean attributes, if tf attributes in reality do notfollow Poisson distributions.2.4Multi-variate Gauss NBThe multi-variate Bernoulli nb can be modified for realvalued attributes, by assuming that each attribute follows anormal distribution g(xi ; µi,c , σi,c ) in each category c, where:−1√g(xi ; µi,c , σi,c ) =eσi,c 2π(xi −µi,c )22σ 2i,c,and the mean (µi,c ) and typical deviation (σi,c ) of each distribution are estimated from the training data. Then, as5In effect, this is a unigram language model. Additionalvariants of the multinomial nb can be formed by using ngram language models instead [20]. See also [13] for otherimprovements that can be made to the multinomial nb.Y g(x ; µmp(~x | c) =ii,c , σi,c ),i=1Qand the criterion for classifying a message as spam becomes:Pp(cs ) ·mi=1c∈{cs ,ch } p(c) ·Qg(xi ; µi,cs , σi,cs )> T.mi=1 g(xi ; µi,c , σi,c )This allows us to use normalized tf attributes, whose values are (non-negative) reals, unlike the tf attributes of themultinomial nb. Real-valued attributes, however, may notfollow normal distributions. With our normalized tf attributes, there is also the problem that negative values arenot used, which leads to a significant loss of probability massin the (presumed) normal distributions of attributes whosevariances are large and means are close to zero.2.5Flexible BayesInstead of using a single normal distribution for each attribute per category, fb models p(xi | c) as the average ofLi,c normal distributions with different mean values, but thesame typical deviation:p(xi | c) =and Nt,c is now the number of occurrences of token t in thetraining messages of category c, while Nc = mi=1 Nti ,c .2.3suming again that the values of the attributes are independent given the category, we get:1·Li,cX g(x ; µLi,cii,c,l , σc ),l=1where Li,c is the number of different values Xi has in thetraining data of category c. Each of these values is used asthe mean µi,c,l of a normal distribution of that category. Allthe distributions of a category c are taken to have the same1, where Mc is againtypical deviation, estimated as σc = √Mcthe number of training messages in c. Hence, the distributions of each category become narrower as more trainingmessages of that category are accumulated; in the case of ournormalized tf attributes, this also alleviates the problem ofprobability mass loss of the multi-variate Gauss nb. Byaveraging several normal distributions, fb can approximatethe true distributions of real-valued attributes more closelythan the multi-variate Gauss nb, when the assumption thatthe attributes follow normal distributions is violated.The computational complexity of all five nb versions isO(m · N ) during training, where N is the total number oftraining messages. At classification time, the computationalcomplexity of the first four versions is O(m), while the complexity of fb is O(m · N ), because of the need to sum theLi distributions. Consult [2] for further details.3.DATASETS AND METHODOLOGYThere has been significant effort to generate public benchmark datasets for anti-spam filtering. One of the main concerns is how to protect the privacy of the users (senders andreceivers) whose ham messages are included in the datasets.The first approach is to use ham messages collected fromfreely accessible newsgroups, or mailing lists with publicarchives. Ling-Spam, the earliest of our benchmark datasets,follows this approach [23]. It consists of spam messages received at the time and ham messages retrieved from thearchives of the Linguist list, a moderated and, hence, spamfree list about linguistics. Ling-Spam has the disadvantage that its ham messages are more topic-specific than themessages most users receive. Hence, it can lead to overoptimistic estimates of the performance of learning-basedspam filters. The SpamAssassin corpus is similar, in thatits ham messages are publicly available; they were collectedfrom public fora, or they were donated by users with the understanding they may be made public.6 Since they were received by different users, however, SpamAssassin’s ham messages are less topic-specific than those a single user wouldreceive. Hence, the resulting dataset is inappropriate forexperimentation with personalized spam filters.An alternative solution to the privacy problem is to distribute information about each message (e.g., the frequencies of particular words in each message), rather than themessages themselves. The Spambase collection follows thisapproach. It consists of vectors, each representing a singlemessage (spam or ham), with each vector containing thevalues of pre-selected attributes, mostly word frequencies.The same approach was adopted in a corpus developed for arecently announced ecml-pkdd 2006 challenge.7 Datasetsthat adopt this approach, however, are much more restrictive than Ling-Spam and the SpamAssassin corpus, becausetheir messages are not available in raw form, and, hence, itis impossible to experiment with attributes other than thosechosen by their creators.A third approach is to release benchmarks each consisting of messages received by a particular user, after replacingeach token by a unique number in all the messages. Themapping between tokens and numbers is not released, making it extremely difficult to recover the original messages,other than perhaps common words and phrases therein. Thisbypasses privacy problems, while producing messages whosetoken sequences are very close, from a statistical point ofview, to the original ones. We have used this encodingscheme in the pu corpora [1, 2, 23]. However, the loss ofthe original tokens still imposes restrictions; for example, itis impossible to experiment with different tokenizers.Following the Enron investigation, the personal files of approximately 150 Enron employees were made publicly available.8 The files included a large number of personal e-mailmessages, which have been used to create e-mail classification benchmarks [3, 15], including a public benchmarkcorpus for the trec 2005 Spam Track.9 During the construction of the latter benchmark, several spam filters wereemployed to weed spam out of the Enron message collection.The collection was then augmented with spam messages collected in 2005, leading to a benchmark with 43,000 ham andapproximately 50,000 spam messages. The 2005 Spam Trackexperiments did not separate the resulting corpus into personal mailboxes, although such a division might have beenpossible via the ‘To:’ field. Hence, the experiments corresponded to the scenario where a single filter is trained on acollection of messages received by many different users, asopposed to using personalized filters.As we were more interested in personalized spam filters,we focussed on six Enron employees who had large mail6The SpamAssassin corpus and Spambase are availablefrom http://www.spamassassin.org/publiccorpus/ andhttp://www.ics.uci.edu/∼mlearn/MLRepository.html.7See http://www.ecmlpkdd2006.org/challenge.html.8See http://fercic.aspensys.com/members/manager.asp.9Consult http://plg.uwaterloo.ca/ gvcormac/spam/ forfurther details. We do not discuss the other three corporaof the 2005 Spam Track, as they are not publicly available.boxes. More specifically, we used the mailboxes of employeesfarmer-d, kaminski-v, kitchen-l, williams-w3, beck-s,and lokay-m, in the cleaned-up form provided by Bekkerman [3], which includes only ham messages.10 We also usedspam messages obtained from four different sources: (1) theSpamAssassin corpus, (2) the Honeypot project,11 (3) thespam collection of Bruce Guenter (bg),12 and spam collectedby the third author of this paper (gp).The first three spam sources above collect spam via traps(e.g., e-mail addresses published on the Web in a way thatmakes it clear to humans, but not to crawlers, that theyshould not be used), resulting in multiple copies of the samemessages. We applied a heuristic to the spam collection weobtained from each one of the first three spam sources, toidentify and remove multiple copies; the heuristic is basedon the number of common text lines in each pair of spammessages. After removing duplicates, we merged the spamcollections obtained from sources 1 and 2, because the messages from source 1 were too few to be used on their ownand did not include recent spam, whereas the messages fromsource 2 were fresher, but they covered a much shorter period of time. The resulting collection (dubbed sh; SpamAssassin spam plus Honeypot) contains messages sent betweenMay 2001 and July 2005. From the third spam source (bg)we kept messages sent between August 2004 and July 2005,a period ending close to the time our datasets were constructed. Finally, the fourth spam source is the only onethat does not rely on traps. It contains all the spam messages received by gp between December 2003 and September2005; duplicates were not removed in this case, as they arepart of a normal stream of incoming spam.The six ham message collections (six Enron users) wereeach paired with one of the three spam collections (sh, bg,gp). Since the vast majority of spam messages are not personalized, we believe that mixing ham messages receivedby one user with spam messages received by others leadsto reasonable benchmarks, provided that additional stepsare taken, as discussed below. The same approach can beused in future to replace the spam messages of our datasetswith fresher ones. We also varied the ham-spam ratios, byrandomly subsampling the spam or ham messages, wherenecessary. In three of the resulting benchmark datasets, weused a ham-spam ratio of approximately 3:1, while in theother three we inverted the ratio to 1:3. The total numberof messages in each dataset is between five and six thousand.The six datasets emulate different situations faced by realusers, allowing us to obtain a more complete picture of theperformance of learning-based filters. Table 1 summarizesthe characteristics of the six datasets. Hereafter, we referto the first, second, . . . , sixth dataset of Table 1 as Enron1,Enron2, . . . , Enron6, respectively.In addition to what was mentioned above, the six datasetswere subjected to the following pre-processing steps. First,we removed messages sent by the owner of the mailbox (wechecked if the address of the owner appeared in the ‘To:’,‘Cc:’, or ‘Bcc:’ fields), since we believe e-mail users are increasingly adopting better ways to keep copies of outgoingmessages. Second, as a simplification, we removed all htmltags and the headers of the messages, keeping only their10The mailboxes can be downloaded from http://www.cs.umass.edu/∼ronb/datasets/enron flat.tar.gz.11Consult http://www.projecthoneypot.org/.12See http://untroubled.org/spam/.Table 1: Composition of the six benchmark datasets.ham + spamfarmer-d + gpkaminski-v + shkitchen-l + bgwilliams-w3 + gpbeck-s + shlokay-m + bgham:spam3672:15004361:14964012:15001500:45001500:36751500:4500Enron1 - ham:spam ratio per batch4.5ham, spam periods[12/99, 1/02], [12/03, 9/05][12/99, 5/01], [5/01, 7/05][2/01, 2/02], [8/04, 7/05][4/01, 2/02], 12/03, 9/05][1/00, 5/01], [5/01, 7/05][6/00, 3/02], [8/04, 7/05]43.532.521.51. Let S and H be the sets of spam and ham messages ofthe dataset, respectively.2. Order the messages of H by time of arrival.3. Insert |S| spam slots between the ordered messages ofH by |S| independent random draws from {1, . . . , |H|}with replacement. If the outcome of a draw is i, a newspam slot is inserted after the i-th ham message. Aham message may thus be followed by several slots.4. Fill the spam slots with the messages of S, by iteratively filling the earliest empty spam slot with theoldest message of S that has not been placed to a slot.The actual dates of the messages are then discarded, andwe assume that the messages (ham and spam) of each dataset494541373329252117913511subjects and bodies. In operational filters, html tags andheaders can provide additional useful attributes, as mentioned above; hence, our datasets lead to conservative estimates of the performance of operational filters. Third, weremoved spam messages written in non-Latin character sets,because the ham messages of our datasets are all written inLatin characters, and, therefore, non-Latin spam messageswould be too easy to identify; i.e., we opted again for harderdatasets, that lead to conservative performance estimates.One of the main goals of our evaluation was to emulatethe situation that a new user of a personalized learningbased anti-spam filter faces: the user starts with a smallamount of training messages, and retrains the filter as newmessages arrive. As noted in [8], this incremental retrainingand evaluation differs significantly from the cross-validationexperiments that are commonly used to measure the performance of learning algorithms, and which have been adoptedin many previous spam filtering experiments, including ourown [2]. There are several reasons for this, including thevarying size of the training set, the increasingly more sophisticated tricks used by spam senders over time, the varying proportion of spam to ham messages in different timeperiods, which makes the estimation of priors difficult, andthe topic shift of spam messages over time. Hence, an incremental retraining and evaluation procedure that also takesinto account the characteristics of spam that vary over timeis essential when comparing different learning algorithms inspam filtering. In order to realize this incremental procedure with the use of our six datasets, we needed to order themessages of each dataset in a way that preserves the originalorder of arrival of the messages in each category; i.e., eachspam message must be preceded by all spam messages thatarrived earlier, and the same applies to ham messages. Thevarying ham-ratio ratio over time also had to be emulated.(The reader is reminded that the spam and ham messagesof each dataset are from different time periods. Hence, onecannot simply use the dates of the messages.) This wasachieved by using the following algorithm in each dataset:batch numberFigure 1: Fluctuation of the ham-spam ratio.arrive in the order produced by the algorithm above. Figure 1 shows the resulting fluctuation of the ham-spam ratioover batches of 100 adjacent messages each. The first batchcontains the “oldest” 100 messages, the second one the 100messages that “arrived” immediately after those of the firstbatch, etc. The ham-spam ratio in the entire dataset is 2.45.In each ordered dataset, the incremental retraining andevaluation procedure was implemented as follows:1. Split the sequence of messages into batches b1 , . . . , blof k adjacent messages each, preserving the order ofarrival. Batch bl may have less than k messages.2. For i = 1 to l − 1, train the filter (including attributeselection) on the messages of batches 1, . . . , i, and testit on the messages of bi+1 .Note that at the end of the evaluation, each message ofthe dataset (excluding b1 ) will have been classified exactlyonce. The number of true positives (TP ) is the number ofspam messages that have been classified as spam, and similarly for false positives (FP , ham misclassified as spam),true negatives (TN , correctly classified ham), and false negatives (FN , spam misclassified as ham). We set k = 100,which emulates the situation where the filter is retrainedevery 100 new messages.13 We assume that the user marksas false negatives spam messages that pass the filter, and inspects periodically for false positives a “spam” folder, wheremessages identified by the filter as spam end up.TP) and hamIn our evaluation, we used spam recall ( TP+FNTNrecall ( TN +FP ). Spam recall is the proportion of spam messages that the filter managed to identify correctly (how muchspam it blocked), whereas ham recall is the proportion ofham messages that passed the filter. Spam recall is the complement of spam misclassification rate, and ham recall thecomplement of ham misclassification rate, the two measuresthat were used in the trec 2005 Spam Track. In order toevaluate the different nb versions across the entire tradeoffbetween true positives and true negatives, we present theevaluation results by means of roc curves, plotting sensitivity (spam recall) against 1− specificity (the complementof ham recall, or ham misclassification rate). This is the13An nb-based filter can easily be retrained on-line, immediately after receiving each new message. We chose k = 100to make it easier to add in future work additional experiments with other learning algorithms, such as svms, whichare computationally more expensive to train.nb versionfbmv Gaussmn tfmv Bernoullimn BooleanEnr17.875.560.882.102.31Enr23.464.750.950.951.97Enr31.431.970.201.092.04Enr41.3112.70.500.450.43Enr50.113.360.751.140.39Enr60.345.270.180.880.20Table 2: Maximum difference (×100) in spam recallacross 500, 1000, 3000 attributes for T = 0.5.nb versionfbmv Gaussmn tfmv Bernoullimn BooleanEnr10.611.172.171.470.53Enr20.230.751.380.630.68Enr31.725.941.026.370.10Enr40.541.770.612.040.48Enr50.485.911.702.111.36Enr60.344.881.221.222.17Table 3: Maximum difference (×100) in ham recallacross 500, 1000, 3000 attributes for T = 0.5.normal definition of roc analysis, when treating spam asthe positive and ham as the negative class.The roc curves capture the overall performance of thedifferent nb versions in each dataset, but fail to providea picture of the progress made by each nb version duringthe incremental procedure. For this reason, we additionallyexamine the learning curves of the five methods in terms ofthe two measures for T = 0.5, i.e., we plot spam and hamrecall as the training set increases during the incrementalretraining and evaluation procedure.4. EXPERIMENTAL RESULTS4.1Size of attribute setWe first examined the impact of the number of attributeson the effectiveness of the five nb versions.14 As mentionedabove, we experimented with 500, 1000, and 3000 attributes.The full results of these experiments (not reported here) indicate that overall the best results are achieved with 3000attributes, as one might have expected. The differences ineffectiveness across different numbers of attributes, however,are rather insignificant. As an example, Tables 2 and 3 showthe maximum differences in spam and ham recall, respectively, across the three sizes of the attribute set, for each nbversion and dataset, with T = 0.5; note that the differencesare in percentage points. The tables show that the differences are very small in all five nb versions for this thresholdvalue, and we obtained very similar results for all thresholds.Consequently, in operational filters the differences in effectiveness may not justify the increased computational costthat larger attribute sets require, even though the increasein computational cost is linear in the number of attributes.4.2Comparison of NB versionsFigure 2 shows the roc curves of the five nb versions ineach one of the six datasets.15 All the curves are for 3000attributes, and the error bars correspond to 0.95 confidenceintervals; we show error bars only at some points to avoid14We used a modified version of filtron [18] for our experiments, with weka’s implementations of the five nb versions;see http://www.cs.waikato.ac.nz/∼ml/weka/.15Please view the figures in color, consulting the on-line version of this paper if necessary; see http://www.ceas.cc/.cluttering the diagrams. Since the tolerance of most userson misclassifying ham messages is very limited, we have restricted the horizontal axis (1 − specificity = 1 − ham recall)of all diagrams to [0, 0.2], i.e., a maximum of 20% of misclassified ham, in order to improve the readability of thediagrams. On the vertical axis (sensitivity, spam recall) weshow the full range, which allows us to examine what proportion of spam messages the five nb versions manage to blockwhen requesting a very low ham misclassification rate (when1−specificity approaches 0). The optimal performance pointin an roc diagram is the top-left corner, while the area under each curve (auc) is often seen as a summary of theperformance of the corresponding method. We do not, however, believe that standard auc is a good measure for spamfilters, because it is dominated by non-high specificity (hamrecall) regions, which are of no interest in practice. Perhapsone should compute the area for 1 − specificity ∈ [0, 0.2]or [0, 0.1]. Even then, however, it is debatable how the areashould be computed when roc curves do not span the entire[0, 0.2] or [0, 0.1] range of the horizontal axis (see below).A first conclusion that can be drawn from the results ofFigure 2 is that some datasets, such as Enron4, are “easier”than others, such as Enron1. There does not seem to be aclear justification for these differences, in terms of the hamspam ratio or the spam source used in each dataset.Despite its theoretical association to term frequencies, inall six datasets the multinomial nb seems to be doing betterwhen Boolean attributes are used, which agrees with Schneider’s observations [25]. The difference, however, is in mostcases very small and not always statistically significant; itis clearer in the first dataset and, to a lesser extent, in thelast one. Furthermore, the multinomial nb with Boolean attributes seems to be the best performer in 4 out of 6 datasets,although again by a small and not always statistically significant margin, and it is clearly outperformed only by fb inthe other 2 datasets. This is particularly interesting, sincemany nb-based spam filters appear to adopt the multinomial nb with tf attributes or the multi-variate Bernoulli nb(which uses Boolean attributes); the latter seems to be theworst among the nb versions we evaluated. Among the nbversions that we tested with normalized tf attributes (fband the multi-variate Gauss nb), overall fb is clearly thebest. However, fb does not always outperform the othernb version that uses non-Boolean attributes, namely themultinomial nb with tf attributes.The fb classifier shows signs of impressive superiority inEnron1 and Enron2; and its performance is almost undistinguishable from that of the top performers in Enron5 andEnron6. However, it does not perform equally well, compared to the top performers, in the other two datasets (Enron3, Enron4), which strangely include what appears to bethe easiest dataset (Enron4). One problem we noticed withfb is that its estimates for p(c | ~x) are very close to 0 or 1;hence, varying the threshold T has no effect on the classification of many messages. This did not allow us to obtainhigher ham recall (lower 1 − specificity) by trading off spamrecall (sensitivity) as well as in the other nb versions, whichis why the fb roc curves are shorter in some of the diagrams.(The same comment applies to the multi-variate Gauss nb.)Having said that, we were able to reach a ham recall levelof 99.9% or higher with fb in most of the datasets.Overall, the multinomial nb with Boolean attributes andfb obtained the best results in our experiments, but the dif-10.90.90.80.8sensitivity (spam recall)sensitivity (spam recall)Enron1 - 3000 Attributes10.70.60.5Flexible Bayes0.4Multivariate NB, Gaussian0.60.5Flexible Bayes0.4Multivariate NB, Gaussian0.3Multinomial NB, TFMultinomial NB, TF0.2Mutivariate NB, binary0.2Mutivariate NB, binary0.1Multinomial NB, binary0.1Multinomial NB, binary0000.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.21 - specificity (1 - ham recall)Enron3 - 3000 Attributes10.90.90.80.80.70.60.5Flexible Bayes0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.21 - specificity (1 - ham recall)Enron4 - 3000 Attributes1sensitivity (spam recall)sensitivity (spam recall)0.70.300.70.60.5Flexible Bayes0.4Multivariate NB, Gaussian0.3Multinomial NB, TF0.4Multivariate NB, Gaussian0.3Multinomial NB, TF0.2Mutivariate NB, binary0.2Mutivariate NB, binaryMultinomial NB, binary0.1Multinomial NB, binary0.10000.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.181 - specificity (1 - ham recall)0.20Enron5 - 3000 Attributes10.90.90.80.80.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.21 - specificity (1 - ham recall)Enron6 - 3000 Attributes1sensitivity (spam recall)sensitivity (spam recall)Enron2 - 3000 Attributes0.70.70.60.60.5Flexible Bayes0.4Multivariate NB, Gaussian0.3Multinomial NB, TF0.5Flexible Bayes0.4Multivariate NB, Gaussian0.3Multinomial NB, TFMutivariate NB, binaryMultinomial NB, binary0.2Mutivariate NB, binary0.20.1Multinomial NB, binary0.10000.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.181 - specificity (1 - ham recall)0.200.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.181 - specificity (1 - ham recall)Figure 2: ROC curves of the five NB versions with 3000 attributes.0.2nb versionfbmv Gaussmn tfmv Bern.mn Bool.Enr190.5093.0895.6697.0896.00Enr293.6395.8096.8191.0596.68Enr396.9497.5595.0497.4296.94Enr495.7880.1497.7997.7097.79Enr599.5695.4299.4297.9599.69Enr699.5591.9598.0897.9298.10Avg.95.9992.3297.1396.5297.53Table 4: Spam recall (%) for 3000 attributes, T = 0.5.nb versionfbmv Gaussmn tfmv Bern.mn Bool.Enr197.6494.8394.0093.1995.25Enr298.8396.9796.7897.2297.83Enr395.3688.8198.8375.4198.88Enr496.6199.3998.3095.8699.05Enr590.7697.2895.6590.0895.65Enr689.9795.8795.1282.5296.88Avg.94.8695.5396.4589.0597.26Table 5: Ham recall (%) for 3000 attributes, T = 0.5.ferences from the other nb versions were often very small.Taking into account its smoother trade-off between ham andspam recall, and its better computational complexity at runtime, we tend to prefer the multinomial nb with Booleanattributes over fb, but further experiments are necessary toestablish its superiority with confidence. For completeness,Tables 4 and 5 list the spam and ham recall, respectively, ofthe nb versions on the 6 datasets for T = 0.5, although comparing at a fixed threshold T is not particularly informative;for example, two methods may obtain the same results atdifferent thresholds. On average, the multinomial nb withBoolean attributes again has the best results, both in spamand ham recall.4.3Learning curvesFigure 3 shows the learning curves (spam and ham recallas more training messages are accumulated over time) of themultinomial nb with Boolean attributes on the six datasetsfor T = 0.5. It is interesting to observe that the curvesdo not increase monotonically, unlike most text classification experiments, presumably because of the unpredictablefluctuation of the ham-spam ratio, the changing topics ofspam, and the adversarial nature of anti-spam filtering. Inthe “easiest” dataset (Enron4) the classifier reaches almostperfect performance, especially in terms of ham recall, aftera few hundreds of messages, and quickly returns to nearperfect performance whenever a deviation occurs. As moretraining messages are accumulated, the deviations from theperfect performance almost disappear. In contrast, in moredifficult datasets (e.g., Enron1) the fluctuation of ham andspam recall is continuous. The classifier seems to adaptquickly to changes, though, avoiding prolonged plateaus oflow performance. Spam recall is particularly high and stablein Enron5, but this comes at the expense of frequent largefluctuations of ham recall; hence, the high spam recall maybe the effect of a tradeoff between spam and ham recall.5.CONCLUSIONS AND FURTHER WORKWe discussed and evaluated experimentally in a spam filtering context five different versions of the Naive Bayes (nb)classifier. Our investigation included two versions of nb thathave not been used widely in the spam filtering literature,namely Flexible Bayes (fb) and the multinomial nb withBoolean attributes. We emulated the situation faced by anew user of a personalized learning-based spam filter, adopt-ing an incremental retraining and evaluation procedure. Thesix datasets that we used, and which we make publicly available, were created by mixing freely available ham and spammessages in different proportions. The mixing procedureemulates the unpredictable fluctuation over time of the hamspam ratio in real mailboxes.Our evaluation included plotting roc curves, which allowed us to compare the different nb versions across theentire tradeoff between true positives and true negatives.The most interesting result of our evaluation was the verygood performance of the two nb versions that have beenused less in spam filtering, i.e., fb and the multinomial nbwith Boolean attributes; these two versions collectively obtained the best results in our experiments. Taking also intoaccount its lower computational complexity at run time andits smoother trade-off between ham and spam recall, we tendto prefer the multinomial nb with Boolean attributes overfb, but further experiments are needed to be confident. Thebest results in terms of effectiveness were generally achievedwith the largest attribute set (3000 attributes), as one mighthave expected, but the gain was rather insignificant, compared to smaller and computationally cheaper attribute sets.We are currently collecting more data, in a setting thatwill allow us to evaluate the five nb versions and other learning algorithms on several real mailboxes with the incremental retraining and evaluation method. The obvious caveat ofthese additional real-user experiments is that it will not bepossible to provide publicly the resulting datasets in a nonencoded form. Therefore, we plan to release them using theencoding scheme of the pu datasets.6.REFERENCES[1] I. Androutsopoulos, J. Koutsias, K. Chandrinos, andC. Spyropoulos. An experimental comparison of NaiveBayesian and keyword-based anti-spam filtering withencrypted personal e-mail messages. In 23rd ACMSIGIR Conference, pages 160–167, Athens, Greece,2000.[2] I. Androutsopoulos, G. Paliouras, and E. Michelakis.Learning to filter unsolicited commercial e-mail.technical report 2004/2, NCSR “Demokritos”, 2004.[3] R. Beckermann, A. McCallum, and G. Huang.Automatic categorization of email into folders:benchmark experiments on Enron and SRI corpora.Technical report IR-418, University of MassachusettsAmherst, 2004.[4] X. Carreras and L. Marquez. Boosting trees foranti-spam email filtering. In 4th InternationalConference on Recent Advances in Natural LanguageProcessing, pages 58–64, Tzigov Chark, Bulgaria,2001.[5] P. Domingos and M. Pazzani. On the optimality of thesimple Bayesian classifier under zero-one loss. MachineLearning, 29(2–3):103130, 1997.[6] H. D. Drucker, D. Wu, and V. Vapnik. Support VectorMachines for spam categorization. IEEE TransactionsOn Neural Networks, 10(5):1048–1054, 1999.[7] S. Eyheramendy, D. Lewis, and D. Madigan. On theNaive Bayes model for text categorization. In 9thInternational Workshop on Artificial Intelligence andStatistics, pages 332–339, Key West, Florida, 2003.[8] T. Fawcett. In “vivo” spam filtering: a challengeEnron2 - Multinomial NB, Boolean - 3000 AttributesEnron1 - Multinomial NB, Boolean - 3000 AttributesEnron3 - Multinomial NB, Boolean - 3000 Attributes1110.950.950.950.90.90.90.850.850.850.8Spam Recall0.75Ham Recall0.7110.950.950.90.90.90.850.850.850.80.75Spam RecallHam Recall0.7Number of emails x 100Number of emails x 10055524946434034312825221916737Ham Recall585552494643403734312825221916494643403734312825221916131077 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 584411Spam Recall0.7130.70.80.7510Ham Recall13Number of emails x 1001Spam RecallHam RecallEnron6 - Multinomial NB, Boolean - 3000 Attributes0.950.810Number of emails x 100Enron5 - Multinomial NB, Boolean - 3000 AttributesEnron4 - Multinomial NB, Boolean - 3000 Attributes0.75458555249464340373431282522191671310 13 16 19 22 25 28 31 34 37 40 43 46 49Number of emails x 1001074411Spam Recall0.710.70.80.757Ham Recall4Spam Recall10.80.75Number of emails x 100Figure 3: Learning curves for the multinomial NB with Boolean attributes and T = 0.5.[9][10][11][12][13][14][15][16][17]problem for KDD. SIGKDD Explorations,5(2):140–148, 2003.S. Hershkop and S. Stolfo. Combining email modelsfor false positive reduction. In 11th ACM SIGKDDConference, pages 98–107, Chicago, Illinois, 2005.J. G. Hidalgo. Evaluating cost-sensitive unsolicitedbulk email categorization. In 17th ACM Symposiumon Applied Computing, pages 615–620, 2002.J. G. Hidalgo and M. M. Lopez. Combining text andheuristics for cost-sensitive spam filtering. In 4thComputational Natural Language Learning Workshop,pages 99–102, Lisbon, Portugal, 2000.J. Hovold. Naive Bayes spam filtering usingword-position-based attributes. In 2nd Conference onEmail and Anti-Spam, Stanford, CA, 2005.J. T. J.D.M. Rennie, L. Shih and D. Karger. Tacklingthe poor assumptions of Naive Bayes classifiers. In20th International Conference on Machine Learning,pages 616–623, Washington, DC, 2003.G. John and P. Langley. Estimating continuousdistributions in Bayesian classifiers. In 11thConference on Uncertainty in Artificial Intelligence,pages 338–345, Montreal, Quebec, 1995.B. Klimt and Y. Yang. The Enron corpus: a newdataset for email classification research. In 15thEuropean Conference on Machine Learning and the8th European Conference on Principles and Practiceof Knowledge Discovery in Databases, pages 217–226,Pisa, Italy, 2004.A. Kolcz and J. Alspector. SVM-based filtering ofe-mail spam with content-specific misclassificationcosts. In Workshop on Text Mining, IEEEInternational Conference on Data Mining, San Jose,California, 2001.A. McCallum and K. Nigam. A comparison of eventmodels for naive bayes text classification. In AAAI’98Workshop on Learning for Text Categorization, pages41–48, Madison, Wisconsin, 1998.[18] E. Michelakis, I. Androutsopoulos, G. Paliouras,G. Sakkis, and P. Stamatopoulos. Filtron: alearning-based anti-spam filter. In 1st Conference onEmail and Anti-Spam, Mountain View, CA, 2004.[19] P. Pantel and D. Lin. SpamCop: a spam classificationand organization program. In Learning for TextCategorization – Papers from the AAAI Workshop,pages 95–98, Madison, Wisconsin, 1998.[20] F. Peng, D. Schuurmans, and S. Wang. Augmentingnaive bayes classifiers with statistical languagemodels. Information Retrieval, 7:317–345, 2004.[21] M. Sahami, S. Dumais, D. Heckerman, andE. Horvitz. A Bayesian approach to filtering junke-mail. In Learning for Text Categorization – Papersfrom the AAAI Workshop, pages 55–62, Madison,Wisconsin, 1998.[22] G. Sakkis, I. Androutsopoulos, G. Paliouras,V. Karkaletsis, C. Spyropoulos, and P. Stamatopoulos.Stacking classifiers for anti-spam filtering of e-mail. InConference on Empirical Methods in NaturalLanguage Processing, pages 44–50, Carnegie MellonUniversity, Pittsburgh, PA, 2001.[23] G. Sakkis, I. Androutsopoulos, G. Paliouras,V. Karkaletsis, C. Spyropoulos, and P. Stamatopoulos.A memory-based approach to anti-spam filtering formailing lists. Information Retrieval, 6(1):49–73, 2003.[24] K.-M. Schneider. A comparison of event models forNaive Bayes anti-spam e-mail filtering. In 10thConference of the European Chapter of the ACL,pages 307–314, Budapest, Hungary, 2003.[25] K.-M. Schneider. On word frequency information andnegative evidence in Naive Bayes text classification. In4th International Conference on Advances in NaturalLanguage Processing, pages 474–485, Alicante, Spain,2004.
Conclusion : S AND FURTHER WORKWe discussed and evaluated experimentally in a spam filtering context five different versions of the Naive Bayes (nb)classifier. Our investigation included two versions of nb thathave not been used widely in the spam filtering literature,namely Flexible Bayes (fb) and the multinomial nb withBoolean attributes. We emulated the situation faced by anew user of a personalized learning-based spam filter, adopt-ing an incremental retraining and evaluation procedure. Thesix datasets that we used, and which we make publicly available, were created by mixing freely available ham and spammessages in different proportions. The mixing procedureemulates the unpredictable fluctuation over time of the hamspam ratio in real mailboxes.Our evaluation included plotting roc curves, which allowed us to compare the different nb versions across theentire tradeoff between true positives and true negatives.The most interesting result of our evaluation was the verygood performance of the two nb versions that have beenused less in spam filtering, i.e., fb and the multinomial nbwith Boolean attributes; these two versions collectively obtained the best results in our experiments. Taking also intoaccount its lower computational complexity at run time andits smoother trade-off between ham and spam recall, we tendto prefer the multinomial nb with Boolean attributes overfb, but further experiments are needed to be confident. Thebest results in terms of effectiveness were generally achievedwith the largest attribute set (3000 attributes), as one mighthave expected, but the gain was rather insignificant, compared to smaller and computationally cheaper attribute sets.We are currently collecting more data, in a setting thatwill allow us to evaluate the five nb versions and other learning algorithms on several real mailboxes with the incremental retraining and evaluation method. The obvious caveat ofthese additional real-user experiments is that it will not bepossible to provide publicly the resulting datasets in a nonencoded form. Therefore, we plan to release them using theencoding scheme of the pu datasets.6.
Discussion :  
Biblio :  [1] I. Androutsopoulos, J. Koutsias, K. Chandrinos, and C. Spyropoulos. An experimental comparison of Naive Bayesian and keyword-based anti-spam filtering with encrypted personal e-mail messages. In 23rd ACM SIGIR Conference, pages 160–167, Athens, Greece, 2000  
  [2] I. Androutsopoulos, G. Paliouras, and E. Michelakis  
  Learning to filter unsolicited commercial e-mail  
  technical report 2004/2, NCSR “Demokritos”, 2004  
  [3] R. Beckermann, A. McCallum, and G. Huang  
  Automatic categorization of email into folders: benchmark experiments on Enron and SRI corpora  
  Technical report IR-418, University of Massachusetts Amherst, 2004  
  [4] X. Carreras and L. Marquez. Boosting trees for anti-spam email filtering. In 4th International Conference on Recent Advances in Natural Language Processing, pages 58–64, Tzigov Chark, Bulgaria, 2001  
  [5] P. Domingos and M. Pazzani. On the optimality of the simple Bayesian classifier under zero-one loss. Machine Learning, 29(2–3):103130, 1997  
  [6] H. D. Drucker, D. Wu, and V. Vapnik. Support Vector Machines for spam categorization. IEEE Transactions On Neural Networks, 10(5):1048–1054, 1999  
  [7] S. Eyheramendy, D. Lewis, and D. Madigan. On the Naive Bayes model for text categorization. In 9th International Workshop on Artificial Intelligence and Statistics, pages 332–339, Key West, Florida, 2003  
  [8] T. Fawcett. In “vivo” spam filtering: a challenge  Enron2 - Multinomial NB, Boolean - 3000 Attributes  Enron1 - Multinomial NB, Boolean - 3000 Attributes  Enron3 - Multinomial NB, Boolean - 3000 Attributes  1  1  1  0.95  0.95  0.95  0.9  0.9  0.9  0.85  0.85  0.85  0.8  Spam Recall  0.75  Ham Recall  0.7  1  1  0.95  0.95  0.9  0.9  0.9  0.85  0.85  0.85 0.8 0.75  Spam Recall  Ham Recall  0.7  Number of emails x 100  Number of emails x 100  55  52  49  46  43  40  34  31  28  25  22  19  16  7  37  Ham Recall 58  55  52  49  46  43  40  37  34  31  28  25  22  19  16  49  46  43  40  37  34  31  28  25  22  19  16  13  10  7  7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58  4  4  1  1  Spam Recall  0.7 13  0.7  0.8 0.75  10  Ham Recall  13  Number of emails x 100  1  Spam Recall  Ham Recall  Enron6 - Multinomial NB, Boolean - 3000 Attributes  0.95  0.8  10  Number of emails x 100  Enron5 - Multinomial NB, Boolean - 3000 Attributes  Enron4 - Multinomial NB, Boolean - 3000 Attributes  0.75  4  58  55  52  49  46  43  40  37  34  31  28  25  22  19  16  7  13  10 13 16 19 22 25 28 31 34 37 40 43 46 49 Number of emails x 100  10  7  4  4  1  1  Spam Recall  0.7 1  0.7  0.8  0.75  7  Ham Recall  4  Spam Recall  1  0.8 0.75  Number of emails x 100  Figure 3: Learning curves for the multinomial NB with Boolean attributes and T = 0.5  
   [9]  [10]  [11]  [12]  [13]  [14]  [15]  [16]  [17]  problem for KDD. SIGKDD Explorations, 5(2):140–148, 2003  
  S. Hershkop and S. Stolfo. Combining email models for false positive reduction. In 11th ACM SIGKDD Conference, pages 98–107, Chicago, Illinois, 2005  
  J. G. Hidalgo. Evaluating cost-sensitive unsolicited bulk email categorization. In 17th ACM Symposium on Applied Computing, pages 615–620, 2002  
  J. G. Hidalgo and M. M. Lopez. Combining text and heuristics for cost-sensitive spam filtering. In 4th Computational Natural Language Learning Workshop, pages 99–102, Lisbon, Portugal, 2000  
  J. Hovold. Naive Bayes spam filtering using word-position-based attributes. In 2nd Conference on Email and Anti-Spam, Stanford, CA, 2005  
  J. T. J.D.M. Rennie, L. Shih and D. Karger. Tackling the poor assumptions of Naive Bayes classifiers. In 20th International Conference on Machine Learning, pages 616–623, Washington, DC, 2003  
  G. John and P. Langley. Estimating continuous distributions in Bayesian classifiers. In 11th Conference on Uncertainty in Artificial Intelligence, pages 338–345, Montreal, Quebec, 1995  
  B. Klimt and Y. Yang. The Enron corpus: a new dataset for email classification research. In 15th European Conference on Machine Learning and the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases, pages 217–226, Pisa, Italy, 2004  
  A. Kolcz and J. Alspector. SVM-based filtering of e-mail spam with content-specific misclassification costs. In Workshop on Text Mining, IEEE International Conference on Data Mining, San Jose, California, 2001  
  A. McCallum and K. Nigam. A comparison of event models for naive bayes text classification. In AAAI’98 Workshop on Learning for Text Categorization, pages 41–48, Madison, Wisconsin, 1998  
   [18] E. Michelakis, I. Androutsopoulos, G. Paliouras, G. Sakkis, and P. Stamatopoulos. Filtron: a learning-based anti-spam filter. In 1st Conference on Email and Anti-Spam, Mountain View, CA, 2004  
  [19] P. Pantel and D. Lin. SpamCop: a spam classification and organization program. In Learning for Text Categorization – Papers from the AAAI Workshop, pages 95–98, Madison, Wisconsin, 1998  
  [20] F. Peng, D. Schuurmans, and S. Wang. Augmenting naive bayes classifiers with statistical language models. Information Retrieval, 7:317–345, 2004  
  [21] M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz. A Bayesian approach to filtering junk e-mail. In Learning for Text Categorization – Papers from the AAAI Workshop, pages 55–62, Madison, Wisconsin, 1998  
  [22] G. Sakkis, I. Androutsopoulos, G. Paliouras, V. Karkaletsis, C. Spyropoulos, and P. Stamatopoulos  
  Stacking classifiers for anti-spam filtering of e-mail. In Conference on Empirical Methods in Natural Language Processing, pages 44–50, Carnegie Mellon University, Pittsburgh, PA, 2001  
  [23] G. Sakkis, I. Androutsopoulos, G. Paliouras, V. Karkaletsis, C. Spyropoulos, and P. Stamatopoulos  
  A memory-based approach to anti-spam filtering for mailing lists. Information Retrieval, 6(1):49–73, 2003  
  [24] K.-M. Schneider. A comparison of event models for Naive Bayes anti-spam e-mail filtering. In 10th Conference of the European Chapter of the ACL, pages 307–314, Budapest, Hungary, 2003  
  [25] K.-M. Schneider. On word frequency information and negative evidence in Naive Bayes text classification. In 4th International Conference on Advances in Natural Language Processing, pages 474–485, Alicante, Spain, 2004  
   